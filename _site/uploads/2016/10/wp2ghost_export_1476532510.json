{"data":{"posts":[{"id":9,"title":"Migrando Banco de Dados para o RDS","slug":"temp-slug-0","markdown":"","html":"","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Fri, 14 Aug 2015 02:48:40 +0000","created_by":1,"updated_at":"Fri, 14 Aug 2015 02:48:40 +0000","updated_by":1,"published_at":"","published_by":1},{"id":14,"title":"Infraestrutura Replic\u00e1vel com CloudFormation","slug":"temp-slug-1","markdown":"","html":"","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Fri, 14 Aug 2015 02:49:23 +0000","created_by":1,"updated_at":"Fri, 14 Aug 2015 02:49:23 +0000","updated_by":1,"published_at":"","published_by":1},{"id":16,"title":"Monitorando os Seus Servidores","slug":"temp-slug-2","markdown":"","html":"","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Fri, 14 Aug 2015 02:49:49 +0000","created_by":1,"updated_at":"Fri, 14 Aug 2015 02:49:49 +0000","updated_by":1,"published_at":"","published_by":1},{"id":23,"title":"Contato","slug":"contato","markdown":"\n[contact-form][contact-field label=\u2019Nome\u2019 type=\u2019name\u2019 required=\u20191\u2019\/][contact-field label=\u2019Email\u2019 type=\u2019email\u2019 required=\u20191\u2019\/][contact-field label=\u2019Coment\u00e1rio\u2019 type=\u2019textarea\u2019 required=\u20191\u2019\/][\/contact-form]\n\n\n","html":"<p>[contact-form][contact-field label=&#8217;Nome&#8217; type=&#8217;name&#8217; required=&#8217;1&#8217;\/][contact-field label=&#8217;Email&#8217; type=&#8217;email&#8217; required=&#8217;1&#8217;\/][contact-field label=&#8217;Coment\u00e1rio&#8217; type=&#8217;textarea&#8217; required=&#8217;1&#8217;\/][\/contact-form]<\/p>\n","image":null,"featured":0,"page":1,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Fri, 14 Aug 2015 23:34:42 +0000","created_by":1,"updated_at":"Sun, 23 Aug 2015 10:39:30 +0000","updated_by":1,"published_at":"Fri, 14 Aug 2015 23:34:42 +0000","published_by":1},{"id":26,"title":"Contact","slug":"contact","markdown":"\n[contact-form][contact-field label=\u2019Name\u2019 type=\u2019name\u2019 required=\u20191\u2019\/][contact-field label=\u2019Email\u2019 type=\u2019email\u2019 required=\u20191\u2019\/][contact-field label=\u2019Comment\u2019 type=\u2019textarea\u2019 required=\u20191\u2019\/][\/contact-form]\n\n\n","html":"<p>[contact-form][contact-field label=&#8217;Name&#8217; type=&#8217;name&#8217; required=&#8217;1&#8217;\/][contact-field label=&#8217;Email&#8217; type=&#8217;email&#8217; required=&#8217;1&#8217;\/][contact-field label=&#8217;Comment&#8217; type=&#8217;textarea&#8217; required=&#8217;1&#8217;\/][\/contact-form]<\/p>\n","image":null,"featured":0,"page":1,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Fri, 14 Aug 2015 23:36:04 +0000","created_by":1,"updated_at":"Sun, 23 Aug 2015 10:39:36 +0000","updated_by":1,"published_at":"Fri, 14 Aug 2015 23:36:04 +0000","published_by":1},{"id":57,"title":"Usando elasticsearch do aws com logstash","slug":"temp-slug-5","markdown":"","html":"","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Sat, 15 Aug 2015 23:26:30 +0000","created_by":1,"updated_at":"Sat, 15 Aug 2015 23:26:30 +0000","updated_by":1,"published_at":"","published_by":1},{"id":59,"title":"Criando um filtro para Logstash","slug":"temp-slug-6","markdown":"","html":"","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Sat, 15 Aug 2015 23:27:16 +0000","created_by":1,"updated_at":"Sat, 15 Aug 2015 23:27:16 +0000","updated_by":1,"published_at":"","published_by":1},{"id":67,"title":"Usando Shield para autentica\u00e7\u00e3o no Kibana","slug":"temp-slug-7","markdown":"","html":"","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Mon, 17 Aug 2015 23:55:46 +0000","created_by":1,"updated_at":"Mon, 17 Aug 2015 23:55:46 +0000","updated_by":1,"published_at":"","published_by":1},{"id":12,"title":"Tutorial - Gerenciando Logs com o Stack ELK","slug":"gerenciando-logs-com-o-stack-elk","markdown":"\nConsolidar, indexar e analisar os logs do seu ambiente\u00a0\u00e9 fundamental. J\u00e1 se foi o tempo em que caminh\u00f5es de logs eram armazenados para s\u00f3 serem usados quando alguma coisa d\u00e1 errado. \u00c9 poss\u00edvel extrair muita informa\u00e7\u00e3o dos logs da sua infraestrutura e aplica\u00e7\u00f5es, permitindo detectar anormalidades rapidamente, antecipar problemas e at\u00e9 suportar decis\u00f5es de neg\u00f3cio.\n\nExistem diversos servi\u00e7os de\u00a0gerenciamento e indexa\u00e7\u00e3o de logs na nuvem.\u00a0[Loggly](http:\/\/www.loggly.com), [Papertrail](http:\/\/papertrailapp.com)\u00a0e\u00a0[Logentries](https:\/\/logentries.com\/)\u00a0s\u00e3o alguns dos mais utilizados. A ideia aqui \u00e9 ser o mais simples poss\u00edvel: crie uma conta e comece a redirecionar os seus logs para o\u00a0*endpoint;\u00a0*o resto \u00e9 com eles.\u00a0J\u00e1 usei os tr\u00eas e algum\u00a0dia vou fazer um comparativo entre eles, mas o objetivo desse post \u00e9 outro: vou\u00a0mostrar como criar um gerenciador de logs\u00a0bastante flex\u00edvel e poderoso utilizando\u00a0Elasticsearch, Logstash e Kibana.\n\n\n## ELK?\n\n[![logos](http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/logos.png)](http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/logos.png)\n\nOs tr\u00eas componentes\u00a0do stack ELK s\u00e3o open source ([licen\u00e7a Apache 2](https:\/\/tldrlegal.com\/license\/apache-license-2.0-(apache-2.0))) e mantidos pela Elastic,\u00a0permitindo modifica\u00e7\u00e3o, distribui\u00e7\u00e3o e uso\u00a0comercial. Os\u00a0componentes s\u00e3o:\n\n**Elasticsearch: **ferramenta distribu\u00edda de busca baseada no Apache Lucene.\u00a0A sua interface \u00e9 uma API\u00a0RESTful com objetos JSON.\n\n**Logstash:\u00a0**ferramenta para cria\u00e7\u00e3o de *pipelines* de dados. O seu objetivo \u00e9 centralizar dados de diversas fontes diferentes, normaliz\u00e1-los e processa-los.\n\n**Kibana:**\u00a0Interface web\u00a0de visualiza\u00e7\u00e3o de dados indexados pelo Elasticsearch. Permite criar gr\u00e1ficos e filtrar dados\u00a0com facilidade.\n\nO fluxo dos dados atrav\u00e9s do stack fica assim: os dados s\u00e3o gerados e enviados para o Logstash. O Logstash l\u00ea, interpreta e filtra esses dados, depois os manda para o Elasticsearch. O Kibana usa o Elasticsearch\u00a0para fazer buscas nos dados e exibi-los de forma amig\u00e1vel.\n\nNesse exemplo, vamos usar um log do Apache como entrada. O fluxo ser\u00e1 o seguinte:\n\n[![logstash-dataflow-1](http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/logstash-dataflow-1.png)](http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/logstash-dataflow-1.png)\n\n\n## Instalando\n\nUsei o\u00a0Centos 7 para esse tutorial.\u00a0Os comandos\u00a0devem funcionar em qualquer distribui\u00e7\u00e3o baseada em RHEL.\n\n### Java\n\nO Logstash funciona tanto com o JDK da Oracle como com o OpenJDK. Vamos usar o\u00a0OpenJDK:  \n`sudo yum install\u00a0java-1.8.0-openjdk.x86_64`  \n O processo de instala\u00e7\u00e3o dos tr\u00eas componentes \u00e9 bem simples: fa\u00e7a o download e descompacte o arquivo. Vamos colocar todas as instala\u00e7\u00f5es na pasta \/opt\/. Usei as vers\u00f5es mais recentes.\n\n### Elasticsearch\n\n`curl -O\u00a0https:\/\/download.elastic.co\/elasticsearch\/elasticsearch\/elasticsearch-1.7.1.tar.gz`  \n`sudo tar -xzvf\u00a0elasticsearch-1.7.1.tar.gz -C \/opt\/`\n\n### Logstash\n\n`curl -O https:\/\/download.elastic.co\/logstash\/logstash\/logstash-1.5.3.tar.gz`  \n`sudo tar -xzvf\u00a0logstash-1.5.3.tar.gz -C \/opt\/`\n\n### Kibana\n\n`curl -O https:\/\/download.elastic.co\/kibana\/kibana\/kibana-4.1.1-linux-x64.tar.gz`  \n`sudo tar -xzvf kibana-4.1.1-linux-x64.tar.gz -C \/opt\/`\n\n\n## Criando\u00a0o\u00a0pipeline do Logstash\n\nPara definir um\u00a0*pipeline*\u00a0de dados no Logstash,\u00a0precisamos determinar\u00a0entradas (*inputs)*\u00a0, filtros (*filters*)\u00a0e sa\u00eddas (*outputs*).\u00a0A fun\u00e7\u00e3o do filtro \u00e9 adaptar o formato da entrada ao formato esperado pela sa\u00edda.\u00a0A arquitetura do Logstash permite\u00a0instalar\u00a0diversos tipos de inputs, filters e outputs na forma de plug-ins, bem como criar os seus pr\u00f3prios plug-ins.\n\nVamos criar um pipeline que tem um log de acesso do Apache como entrada e o\u00a0Elasticsearch como sa\u00edda. Temos ent\u00e3o o seguinte cen\u00e1rio:\n\n**input:** l\u00ea um arquivo contendo o log de acesso do Apache\n\n**filter:\u00a0**transforma cada linha do log em um JSON\u00a0para ser usado como entrada no Elasticsearch\n\n**output:\u00a0**envia o JSON\u00a0gerado pelo filtro para o Elasticsearch\n\nVamos usar o [filtro Grok](https:\/\/www.elastic.co\/guide\/en\/logstash\/current\/plugins-filters-grok.html)\u00a0para transformar o texto do arquivo de log em dado estruturado. O Grok permite parsear entradas de texto usando *patterns*. Ele j\u00e1 vem com [diversos *patterns*](https:\/\/github.com\/logstash-plugins\/logstash-patterns-core\/tree\/master\/patterns), e o log do Apache \u00e9 um deles.\n\nEm linhas gerais, o input file transforma cada linha do arquivo de log em um evento. O\u00a0filtro do Grok l\u00ea\u00a0as linhas, encontra os campos (ip, tipo de requisi\u00e7\u00e3o, data etc.) e os adiciona\u00a0ao evento. O output do Elasticsearch recebe esse evento, transforma os campos em um JSON e envia para uma inst\u00e2ncia do Elasticsearch.\n\nO arquivo de configura\u00e7\u00e3o fica assim:  \n<style>.gist table { margin-bottom: 0; }<\/style>\n\n<div class=\"gist-oembed\" data-gist=\"f05215d5da371205fb6a.json\"><\/div>Mude o \/caminho\/para\/apache.log para (adivinha \u2026) o caminho do log do Apache na sua m\u00e1quina e salve o arquivo de configura\u00e7\u00e3o na raiz da instala\u00e7\u00e3o do Logstash.\u00a0Se voc\u00ea n\u00e3o tem um arquivo de log do apache para usar como entrada, use [esse aqui](http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/access_log.tgz). Basta descompactar com `tar -xzvf access_log.tgz`\n\nVoc\u00ea deve ter percebido que, diferentemente do que se esperava, o output elasticsearch n\u00e3o especifica\u00a0o endere\u00e7o da\u00a0inst\u00e2ncia do Elasticsearch. Como a inst\u00e2ncia estar\u00e1 rodando na mesma m\u00e1quina do Logstash e vem\u00a0configurada com o *Multicast* habilitado por padr\u00e3o, ela ser\u00e1 encontrada automaticamente. Essa configura\u00e7\u00e3o\u00a0**n\u00e3o \u00e9 recomendada para ambientes de produ\u00e7\u00e3o**.\n\n\n## Rodando\n\nVamos come\u00e7ar inicializando o elasticsearch:\n\n`<raiz_do_elasticsearch>\/bin\/elasticsearch -d`\n\nAgora o Logstash:\n\n`<raiz_do_logstash>\/bin\/logstash agent -f pipeline.conf &`\n\nE por \u00faltimo o Kibana.\u00a0Repare que n\u00e3o precisamos mudar\u00a0nenhuma configura\u00e7\u00e3o: o Kibana vai procurar o elasticsearch no localhost:9200 por padr\u00e3o.\n\n`<raiz_do_kibana>\/bin\/kibana &`\n\nAgora abra um browser e acesse a porta 5601 da m\u00e1quina. Voc\u00ea\u00a0vai ver algo assim:\n\n[![kibana-dashboard](http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/Screen-Shot-2015-08-18-at-10.27.34-PM.png)](http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/Screen-Shot-2015-08-18-at-10.27.34-PM.png)\n\nPronto! Voc\u00ea tem um gerenciador de logs\u00a0open source, flex\u00edvel\u00a0e escal\u00e1vel nas suas m\u00e3os. Escrevi um pouco mais sobre o uso do Kibana [nesse post](http:\/\/rafaelmt.github.io\/pt\/2015\/08\/25\/tutorial-kibana\/).\n\n\u00a0\n\n[yasr_visitor_votes size=\u201dsmall\u201d]\n\n\n","html":"<p>Consolidar, indexar e analisar os logs do seu ambiente\u00a0\u00e9 fundamental. J\u00e1 se foi o tempo em que caminh\u00f5es de logs eram armazenados para s\u00f3 serem usados quando alguma coisa d\u00e1 errado. \u00c9 poss\u00edvel extrair muita informa\u00e7\u00e3o dos logs da sua infraestrutura e aplica\u00e7\u00f5es, permitindo detectar anormalidades rapidamente, antecipar problemas e at\u00e9 suportar decis\u00f5es de neg\u00f3cio.<\/p>\n<p>Existem diversos servi\u00e7os de\u00a0gerenciamento e indexa\u00e7\u00e3o de logs na nuvem.\u00a0<a href=\"http:\/\/www.loggly.com\" target=\"_blank\">Loggly<\/a>, <a href=\"http:\/\/papertrailapp.com\" target=\"_blank\">Papertrail<\/a>\u00a0e\u00a0<a href=\"https:\/\/logentries.com\/\" target=\"_blank\">Logentries<\/a>\u00a0s\u00e3o alguns dos mais utilizados. A ideia aqui \u00e9 ser o mais simples poss\u00edvel: crie uma conta e comece a redirecionar os seus logs para o\u00a0<em>endpoint;\u00a0<\/em>o resto \u00e9 com eles.\u00a0J\u00e1 usei os tr\u00eas e algum\u00a0dia vou fazer um comparativo entre eles, mas o objetivo desse post \u00e9 outro: vou\u00a0mostrar como criar um gerenciador de logs\u00a0bastante flex\u00edvel e poderoso utilizando\u00a0Elasticsearch, Logstash e Kibana.<\/p>\n<p><!--more--><\/p>\n<h2>ELK?<\/h2>\n<p><a href=\"http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/logos.png\"><img class=\" wp-image-76 aligncenter\" src=\"http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/logos.png\" alt=\"logos\" width=\"602\" height=\"274\" \/><\/a><\/p>\n<p>Os tr\u00eas componentes\u00a0do stack ELK s\u00e3o open source (<a href=\"https:\/\/tldrlegal.com\/license\/apache-license-2.0-(apache-2.0)\">licen\u00e7a Apache 2<\/a>) e mantidos pela Elastic,\u00a0permitindo modifica\u00e7\u00e3o, distribui\u00e7\u00e3o e uso\u00a0comercial. Os\u00a0componentes s\u00e3o:<\/p>\n<p><strong>Elasticsearch: <\/strong>ferramenta distribu\u00edda de busca baseada no Apache Lucene.\u00a0A sua interface \u00e9 uma API\u00a0RESTful com objetos JSON.<\/p>\n<p><strong>Logstash:\u00a0<\/strong>ferramenta para cria\u00e7\u00e3o de <em>pipelines<\/em> de dados. O seu objetivo \u00e9 centralizar dados de diversas fontes diferentes, normaliz\u00e1-los e processa-los.<\/p>\n<p><strong>Kibana:<\/strong>\u00a0Interface web\u00a0de visualiza\u00e7\u00e3o de dados indexados pelo Elasticsearch. Permite criar gr\u00e1ficos e filtrar dados\u00a0com facilidade.<\/p>\n<p>O fluxo dos dados atrav\u00e9s do stack fica assim: os dados s\u00e3o gerados e enviados para o Logstash. O Logstash l\u00ea, interpreta e filtra esses dados, depois os manda para o Elasticsearch. O Kibana usa o Elasticsearch\u00a0para fazer buscas nos dados e exibi-los de forma amig\u00e1vel.<\/p>\n<p>Nesse exemplo, vamos usar um log do Apache como entrada. O fluxo ser\u00e1 o seguinte:<\/p>\n<p><a href=\"http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/logstash-dataflow-1.png\"><img class=\" wp-image-74 aligncenter\" src=\"http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/logstash-dataflow-1.png\" alt=\"logstash-dataflow-1\" width=\"496\" height=\"293\" \/><\/a><\/p>\n<h2>Instalando<\/h2>\n<p>Usei o\u00a0Centos 7 para esse tutorial.\u00a0Os comandos\u00a0devem funcionar em qualquer distribui\u00e7\u00e3o baseada em RHEL.<\/p>\n<h3>Java<\/h3>\n<p>O Logstash funciona tanto com o JDK da Oracle como com o OpenJDK. Vamos usar o\u00a0OpenJDK:<br \/>\n<code>sudo yum install\u00a0java-1.8.0-openjdk.x86_64<\/code><br \/>\nO processo de instala\u00e7\u00e3o dos tr\u00eas componentes \u00e9 bem simples: fa\u00e7a o download e descompacte o arquivo. Vamos colocar todas as instala\u00e7\u00f5es na pasta \/opt\/. Usei as vers\u00f5es mais recentes.<\/p>\n<h3>Elasticsearch<\/h3>\n<p><code>curl -O\u00a0https:\/\/download.elastic.co\/elasticsearch\/elasticsearch\/elasticsearch-1.7.1.tar.gz<\/code><br \/>\n<code>sudo tar -xzvf\u00a0elasticsearch-1.7.1.tar.gz -C \/opt\/<\/code><\/p>\n<h3>Logstash<\/h3>\n<p><code>curl -O https:\/\/download.elastic.co\/logstash\/logstash\/logstash-1.5.3.tar.gz<\/code><br \/>\n<code>sudo tar -xzvf\u00a0logstash-1.5.3.tar.gz -C \/opt\/<\/code><\/p>\n<h3>Kibana<\/h3>\n<p><code>curl -O https:\/\/download.elastic.co\/kibana\/kibana\/kibana-4.1.1-linux-x64.tar.gz<\/code><br \/>\n<code>sudo tar -xzvf kibana-4.1.1-linux-x64.tar.gz -C \/opt\/<\/code><\/p>\n<h2>Criando\u00a0o\u00a0pipeline do Logstash<\/h2>\n<p>Para definir um\u00a0<em>pipeline<\/em>\u00a0de dados no Logstash,\u00a0precisamos determinar\u00a0entradas (<em>inputs)<\/em>\u00a0, filtros (<em>filters<\/em>)\u00a0e sa\u00eddas (<em>outputs<\/em>).\u00a0A fun\u00e7\u00e3o do filtro \u00e9 adaptar o formato da entrada ao formato esperado pela sa\u00edda.\u00a0A arquitetura do Logstash permite\u00a0instalar\u00a0diversos tipos de inputs, filters e outputs na forma de plug-ins, bem como criar os seus pr\u00f3prios plug-ins.<\/p>\n<p>Vamos criar um pipeline que tem um log de acesso do Apache como entrada e o\u00a0Elasticsearch como sa\u00edda. Temos ent\u00e3o o seguinte cen\u00e1rio:<\/p>\n<p><strong>input:<\/strong> l\u00ea um arquivo contendo o log de acesso do Apache<\/p>\n<p><strong>filter:\u00a0<\/strong>transforma cada linha do log em um JSON\u00a0para ser usado como entrada no Elasticsearch<\/p>\n<p><strong>output:\u00a0<\/strong>envia o JSON\u00a0gerado pelo filtro para o Elasticsearch<\/p>\n<p>Vamos usar o <a href=\"https:\/\/www.elastic.co\/guide\/en\/logstash\/current\/plugins-filters-grok.html\" target=\"_blank\">filtro Grok<\/a>\u00a0para transformar o texto do arquivo de log em dado estruturado. O Grok permite parsear entradas de texto usando <em>patterns<\/em>. Ele j\u00e1 vem com <a href=\"https:\/\/github.com\/logstash-plugins\/logstash-patterns-core\/tree\/master\/patterns\" target=\"_blank\">diversos <em>patterns<\/em><\/a>, e o log do Apache \u00e9 um deles.<\/p>\n<p>Em linhas gerais, o input file transforma cada linha do arquivo de log em um evento. O\u00a0filtro do Grok l\u00ea\u00a0as linhas, encontra os campos (ip, tipo de requisi\u00e7\u00e3o, data etc.) e os adiciona\u00a0ao evento. O output do Elasticsearch recebe esse evento, transforma os campos em um JSON e envia para uma inst\u00e2ncia do Elasticsearch.<\/p>\n<p>O arquivo de configura\u00e7\u00e3o fica assim:<br \/>\n<style>.gist table { margin-bottom: 0; }<\/style><div class=\"gist-oembed\" data-gist=\"f05215d5da371205fb6a.json\"><\/div><\/p>\n<p>Mude o \/caminho\/para\/apache.log para (adivinha &#8230;) o caminho do log do Apache na sua m\u00e1quina e salve o arquivo de configura\u00e7\u00e3o na raiz da instala\u00e7\u00e3o do Logstash.\u00a0Se voc\u00ea n\u00e3o tem um arquivo de log do apache para usar como entrada, use <a href=\"http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/access_log.tgz\">esse aqui<\/a>. Basta descompactar com <code>tar -xzvf access_log.tgz<\/code><\/p>\n<p>Voc\u00ea deve ter percebido que, diferentemente do que se esperava, o output elasticsearch n\u00e3o especifica\u00a0o endere\u00e7o da\u00a0inst\u00e2ncia do Elasticsearch. Como a inst\u00e2ncia estar\u00e1 rodando na mesma m\u00e1quina do Logstash e vem\u00a0configurada com o <em>Multicast<\/em> habilitado por padr\u00e3o, ela ser\u00e1 encontrada automaticamente. Essa configura\u00e7\u00e3o\u00a0<strong>n\u00e3o \u00e9 recomendada para ambientes de produ\u00e7\u00e3o<\/strong>.<\/p>\n<h2>Rodando<\/h2>\n<p>Vamos come\u00e7ar inicializando o elasticsearch:<\/p>\n<p><code>&lt;raiz_do_elasticsearch&gt;\/bin\/elasticsearch -d<\/code><\/p>\n<p>Agora o Logstash:<\/p>\n<p><code>&lt;raiz_do_logstash&gt;\/bin\/logstash agent -f pipeline.conf &amp;<\/code><\/p>\n<p>E por \u00faltimo o Kibana.\u00a0Repare que n\u00e3o precisamos mudar\u00a0nenhuma configura\u00e7\u00e3o: o Kibana vai procurar o elasticsearch no localhost:9200 por padr\u00e3o.<\/p>\n<p><code>&lt;raiz_do_kibana&gt;\/bin\/kibana &amp;<\/code><\/p>\n<p>Agora abra um browser e acesse a porta 5601 da m\u00e1quina. Voc\u00ea\u00a0vai ver algo assim:<\/p>\n<p><a href=\"http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/Screen-Shot-2015-08-18-at-10.27.34-PM.png\"><img class=\"wp-image-71 aligncenter\" src=\"http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/Screen-Shot-2015-08-18-at-10.27.34-PM.png\" alt=\"kibana-dashboard\" width=\"717\" height=\"351\" \/><\/a><\/p>\n<p>Pronto! Voc\u00ea tem um gerenciador de logs\u00a0open source, flex\u00edvel\u00a0e escal\u00e1vel nas suas m\u00e3os. Escrevi um pouco mais sobre o uso do Kibana <a href=\"http:\/\/rafaelmt.github.io\/pt\/2015\/08\/25\/tutorial-kibana\/\">nesse post<\/a>.<\/p>\n<p>&nbsp;<\/p>\n<p>[yasr_visitor_votes size=&#8221;small&#8221;]<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 18 Aug 2015 23:54:58 +0000","created_by":1,"updated_at":"Fri, 04 Sep 2015 15:13:49 +0000","updated_by":1,"published_at":"Tue, 18 Aug 2015 23:54:58 +0000","published_by":1},{"id":85,"title":"Tutorial - Managing Logs with the ELK Stack","slug":"managing-logs-with-the-elk-stack","markdown":"\nConsolidating, indexing and analyzing your environment\u2019s logs is fundamental. Gone are the days when truckloads of logs were stored only to be used when something went wrong. It\u2019s possible to extract a lot of information from your infrastructure and applications\u2019 logs, allowing you to rapidly detect abnormalities, foresee problems and even support business decisions.\n\nThere are plenty of cloud log\u00a0management services around:\u00a0[Loggly](http:\/\/www.loggly.com), [Papertrail](http:\/\/papertrailapp.com)\u00a0e\u00a0[Logentries](https:\/\/logentries.com\/)\u00a0are some of the most used. Their goal is to be as simple as\u00a0it gets: create an account, redirect your logs to the provided endpoint and that\u2019s it: the rest is on their side. I\u2019ve used these three and I might write about them some day, but we\u00a0have another objective in this post: I\u2019ll show how you can create a flexible and powerful log manager using the Elasticsearch\/Logstash\/Kibana stack.\n\n\n## ELK?\n\n[![logos](http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/logos1.png)](http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/logos1.png)\n\nAll three components of the ELK stack are open source ([Apache 2 license](https:\/\/tldrlegal.com\/license\/apache-license-2.0-(apache-2.0))) and kept by Elastic, allowing modification, distribution and commercial use. The components are:\n\n**Elasticsearch: **distributed search engine based on Apache Lucene. It has a RESTful API as interface, with JSON objects.\n\n**Logstash: **data pipeline tool. Centralizes, normalizes and processes data from different sources.\n\n**Kibana:**\u00a0data visualization web interface. Allows creating graphs and filters for data indexed by Elasticsearch.\n\nThis is the data flow throughout the stack: data is generated and sent to Logstash. Logstash reads, parses and filters the data, then sends it to Elasticsearch. Kibana uses Elasticsearch\u2019s \u00a0indexes to search data and displays it in a user-friendly way.\n\nIn this example, we\u2019ll use an Apache log as input. The flow is as follows:\n\n\u00a0\n\n[![logstash-dataflow-1](http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/logstash-dataflow-1.png)](http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/logstash-dataflow-1.png)\n\n\n## Installation\n\nI\u2019ve used Centos 7 for this tutorial. The commands should work in any RHEL-based distro.\n\n### Java\n\nLogstash runs both on Oracle JDK and OpenJDK. We\u2019ll use OpenJDK:\n\n`sudo yum install\u00a0java-1.8.0-openjdk.x86_64`\n\nThe stack installation process is pretty straightforward: simply download the components and decompress the files. I\u2019ll put them all in the\u00a0\/opt\/ folder. I\u2019m using the most recent versions (at the time of writing).\n\n### Elasticsearch\n\n`curl -O\u00a0https:\/\/download.elastic.co\/elasticsearch\/elasticsearch\/elasticsearch-1.7.1.tar.gz`  \n`sudo tar -xzvf\u00a0elasticsearch-1.7.1.tar.gz -C \/opt\/`\n\n### Logstash\n\n`curl -O https:\/\/download.elastic.co\/logstash\/logstash\/logstash-1.5.3.tar.gz`  \n`sudo tar -xzvf\u00a0logstash-1.5.3.tar.gz -C \/opt\/`\n\n### Kibana\n\n`curl -O https:\/\/download.elastic.co\/kibana\/kibana\/kibana-4.1.1-linux-x64.tar.gz`  \n`sudo tar -xzvf kibana-4.1.1-linux-x64.tar.gz -C \/opt\/`\n\n\n## Creating the Logstash pipeline\n\nTo create a Logstash data pipeline, we need to specify the\u00a0inputs, filters and outputs. \u00a0The inputs and outputs\u00a0read and write data, while the filters adapt the data format to what the output\u00a0expects. Logstash\u2019s architecture allows installing inputs, filters and outputs as plugins, as well as creating your own.\n\nLet\u2019s create a pipeline with a an Apache access log as input and Elasticsearch as output. The scenario is as follows:\n\n**input:**\u00a0reads a file containing Apache access log\n\n**filter:\u00a0**parses\u00a0each\u00a0log line in a JSON object to be used as input in Elasticsearch\n\n**output: **sends the JSON objects to an Elasticsearch instance\n\nWe\u2019ll use the [Grok filter](https:\/\/www.elastic.co\/guide\/en\/logstash\/current\/plugins-filters-grok.html) to transform text from the log file into structured data. Grok allows parsing input text using patterns. It ships with a number of [different patterns](https:\/\/github.com\/logstash-plugins\/logstash-patterns-core\/tree\/master\/patterns), Apache being one of them.\n\nThe *file* input transforms each line of the log file in an event. The Grok filter reads the events, finds the fields matching the pattern (ip, request type, response etc.) and adds them to the event. The Elasticsearch output gets the events, transforms them into JSON objects and sends it to an Elasticsearch instance.\n\nThis is the configuration file for the pipeline:  \n<style>.gist table { margin-bottom: 0; }<\/style>\n\n<div class=\"gist-oembed\" data-gist=\"4e0c4561471f8a00f428.json\"><\/div>Change \/path\/to\/apache_log to (guess what \u2026) the path where apache log is in your machine and save it. If you don\u2019t have an Apache log to test, use [this one](http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/access_log.tgz). Decompress it with\u00a0`tar -xzvf access_log.tgz`\n\nYou might have noticed that, contrary to what\u00a0one could expect, the elasticsearch output does not specify the Elasticsearch\u00a0instance\u2019s address. Since Elasticsearch is running in the same machine as Logstash and is configured with Multicast enabled by default, it will be automatically found. This configuration is **not recommended for production environments.**\n\n\n## Running\n\nLet\u2019s start by initializing\u00a0Elasticsearch:\n\n`<elasticsearch_root_folder>\/bin\/elasticsearch -d`\n\nNow Logstash:\n\n`<logstash_root_folder>\/bin\/logstash -f pipeline.conf &`\n\nAnd now Kibana. Notice that we didn\u2019t edit any configuration files for Kibana: by default, it will look for an Elasticsearch instance in\u00a0localhost:9200 (which is exactly where our elasticsearch is).\n\n`<kibana_root_folder>\/bin\/kibana &`\n\nNow browse to the port 5601 of your machine.\u00a0You should see something like this:\n\n[![kibana-dashboard](http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/Screen-Shot-2015-08-18-at-10.27.34-PM.png)](http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/Screen-Shot-2015-08-18-at-10.27.34-PM.png)\n\nThere you go! You have an open source, flexible and scalable log management system.\u00a0You can read a little more about Kibana usage [in this post](http:\/\/rafaelmt.github.io\/en\/2015\/09\/01\/kibana-tutorial\/).\n\n\u00a0\n\n[yasr_visitor_votes size=\u201dsmall\u201d]\n\n\n","html":"<p>Consolidating, indexing and analyzing your environment&#8217;s logs is fundamental. Gone are the days when truckloads of logs were stored only to be used when something went wrong. It&#8217;s possible to extract a lot of information from your infrastructure and applications&#8217; logs, allowing you to rapidly detect abnormalities, foresee problems and even support business decisions.<\/p>\n<p>There are plenty of cloud log\u00a0management services around:\u00a0<a href=\"http:\/\/www.loggly.com\" target=\"_blank\">Loggly<\/a>, <a href=\"http:\/\/papertrailapp.com\" target=\"_blank\">Papertrail<\/a>\u00a0e\u00a0<a href=\"https:\/\/logentries.com\/\" target=\"_blank\">Logentries<\/a>\u00a0are some of the most used. Their goal is to be as simple as\u00a0it gets: create an account, redirect your logs to the provided endpoint and that&#8217;s it: the rest is on their side. I&#8217;ve used these three and I might write about them some day, but we\u00a0have another objective in this post: I&#8217;ll show how you can create a flexible and powerful log manager using the Elasticsearch\/Logstash\/Kibana stack.<\/p>\n<p><!--more--><\/p>\n<h2>ELK?<\/h2>\n<p><a href=\"http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/logos1.png\"><img class=\"wp-image-119 aligncenter\" src=\"http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/logos1.png\" alt=\"logos\" width=\"578\" height=\"263\" \/><\/a><\/p>\n<p>All three components of the ELK stack are open source (<a href=\"https:\/\/tldrlegal.com\/license\/apache-license-2.0-(apache-2.0)\">Apache 2 license<\/a>) and kept by Elastic, allowing modification, distribution and commercial use. The components are:<\/p>\n<p><strong>Elasticsearch: <\/strong>distributed search engine based on Apache Lucene. It has a RESTful API as interface, with JSON objects.<\/p>\n<p><strong>Logstash: <\/strong>data pipeline tool. Centralizes, normalizes and processes data from different sources.<\/p>\n<p><strong>Kibana:<\/strong>\u00a0data visualization web interface. Allows creating graphs and filters for data indexed by Elasticsearch.<\/p>\n<p>This is the data flow throughout the stack: data is generated and sent to Logstash. Logstash reads, parses and filters the data, then sends it to Elasticsearch. Kibana uses Elasticsearch&#8217;s \u00a0indexes to search data and displays it in a user-friendly way.<\/p>\n<p>In this example, we&#8217;ll use an Apache log as input. The flow is as follows:<\/p>\n<p>&nbsp;<\/p>\n<p><a href=\"http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/logstash-dataflow-1.png\"><img class=\"size-full wp-image-74 aligncenter\" src=\"http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/logstash-dataflow-1.png\" alt=\"logstash-dataflow-1\" width=\"523\" height=\"309\" \/><\/a><\/p>\n<h2>Installation<\/h2>\n<p>I&#8217;ve used Centos 7 for this tutorial. The commands should work in any RHEL-based distro.<\/p>\n<h3>Java<\/h3>\n<p>Logstash runs both on Oracle JDK and OpenJDK. We&#8217;ll use OpenJDK:<\/p>\n<p><code>sudo yum install\u00a0java-1.8.0-openjdk.x86_64<\/code><\/p>\n<p>The stack installation process is pretty straightforward: simply download the components and decompress the files. I&#8217;ll put them all in the\u00a0\/opt\/ folder. I&#8217;m using the most recent versions (at the time of writing).<\/p>\n<h3>Elasticsearch<\/h3>\n<p><code>curl -O\u00a0https:\/\/download.elastic.co\/elasticsearch\/elasticsearch\/elasticsearch-1.7.1.tar.gz<\/code><br \/>\n<code>sudo tar -xzvf\u00a0elasticsearch-1.7.1.tar.gz -C \/opt\/<\/code><\/p>\n<h3>Logstash<\/h3>\n<p><code>curl -O https:\/\/download.elastic.co\/logstash\/logstash\/logstash-1.5.3.tar.gz<\/code><br \/>\n<code>sudo tar -xzvf\u00a0logstash-1.5.3.tar.gz -C \/opt\/<\/code><\/p>\n<h3>Kibana<\/h3>\n<p><code>curl -O https:\/\/download.elastic.co\/kibana\/kibana\/kibana-4.1.1-linux-x64.tar.gz<\/code><br \/>\n<code>sudo tar -xzvf kibana-4.1.1-linux-x64.tar.gz -C \/opt\/<\/code><\/p>\n<h2>Creating the Logstash pipeline<\/h2>\n<p>To create a Logstash data pipeline, we need to specify the\u00a0inputs, filters and outputs. \u00a0The inputs and outputs\u00a0read and write data, while the filters adapt the data format to what the output\u00a0expects. Logstash&#8217;s architecture allows installing inputs, filters and outputs as plugins, as well as creating your own.<\/p>\n<p>Let&#8217;s create a pipeline with a an Apache access log as input and Elasticsearch as output. The scenario is as follows:<\/p>\n<p><strong>input:<\/strong>\u00a0reads a file containing Apache access log<\/p>\n<p><strong>filter:\u00a0<\/strong>parses\u00a0each\u00a0log line in a JSON object to be used as input in Elasticsearch<\/p>\n<p><strong>output: <\/strong>sends the JSON objects to an Elasticsearch instance<\/p>\n<p>We&#8217;ll use the <a href=\"https:\/\/www.elastic.co\/guide\/en\/logstash\/current\/plugins-filters-grok.html\" target=\"_blank\">Grok filter<\/a> to transform text from the log file into structured data. Grok allows parsing input text using patterns. It ships with a number of <a href=\"https:\/\/github.com\/logstash-plugins\/logstash-patterns-core\/tree\/master\/patterns\" target=\"_blank\">different patterns<\/a>, Apache being one of them.<\/p>\n<p>The <em>file<\/em> input transforms each line of the log file in an event. The Grok filter reads the events, finds the fields matching the pattern (ip, request type, response etc.) and adds them to the event. The Elasticsearch output gets the events, transforms them into JSON objects and sends it to an Elasticsearch instance.<\/p>\n<p>This is the configuration file for the pipeline:<br \/>\n<style>.gist table { margin-bottom: 0; }<\/style><div class=\"gist-oembed\" data-gist=\"4e0c4561471f8a00f428.json\"><\/div><\/p>\n<p>Change \/path\/to\/apache_log to (guess what &#8230;) the path where apache log is in your machine and save it. If you don&#8217;t have an Apache log to test, use <a href=\"http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/access_log.tgz\">this one<\/a>. Decompress it with\u00a0<code>tar -xzvf access_log.tgz<\/code><\/p>\n<p>You might have noticed that, contrary to what\u00a0one could expect, the elasticsearch output does not specify the Elasticsearch\u00a0instance&#8217;s address. Since Elasticsearch is running in the same machine as Logstash and is configured with Multicast enabled by default, it will be automatically found. This configuration is <strong>not recommended for production environments.<\/strong><\/p>\n<h2>Running<\/h2>\n<p>Let&#8217;s start by initializing\u00a0Elasticsearch:<\/p>\n<p><code>&lt;elasticsearch_root_folder&gt;\/bin\/elasticsearch -d<\/code><\/p>\n<p>Now Logstash:<\/p>\n<p><code>&lt;logstash_root_folder&gt;\/bin\/logstash -f pipeline.conf &amp;<\/code><\/p>\n<p>And now Kibana. Notice that we didn&#8217;t edit any configuration files for Kibana: by default, it will look for an Elasticsearch instance in\u00a0localhost:9200 (which is exactly where our elasticsearch is).<\/p>\n<p><code>&lt;kibana_root_folder&gt;\/bin\/kibana &amp;<\/code><\/p>\n<p>Now browse to the port 5601 of your machine.\u00a0You should see something like this:<\/p>\n<p><a href=\"http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/Screen-Shot-2015-08-18-at-10.27.34-PM.png\"><img class=\"alignnone size-full wp-image-71\" src=\"http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/Screen-Shot-2015-08-18-at-10.27.34-PM.png\" alt=\"kibana-dashboard\" width=\"1280\" height=\"627\" \/><\/a><\/p>\n<p>There you go! You have an open source, flexible and scalable log management system.\u00a0You can read a little more about Kibana usage <a href=\"http:\/\/rafaelmt.github.io\/en\/2015\/09\/01\/kibana-tutorial\/\">in this post<\/a>.<\/p>\n<p>&nbsp;<\/p>\n<p>[yasr_visitor_votes size=&#8221;small&#8221;]<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Wed, 19 Aug 2015 23:53:32 +0000","created_by":1,"updated_at":"Fri, 04 Sep 2015 15:13:40 +0000","updated_by":1,"published_at":"Wed, 19 Aug 2015 23:53:32 +0000","published_by":1},{"id":92,"title":"Tutorial Kibana","slug":"tutorial-kibana","markdown":"\nNo post\u00a0[Gerenciando Logs com o Stack ELK](http:\/\/rafaelmt.github.io\/pt\/2015\/08\/18\/gerenciando-logs-com-o-stack-elk\/)\u00a0instalamos e configuramos um Stack ELK para\u00a0consolidar e analizar logs de um Servidor Apache.\u00a0Nesse post vou falar um pouco mais do Kibana e como\u00a0us\u00e1-lo para\u00a0criar gr\u00e1ficos e filtros para esses dados.\n\n\u00a0\n\n\n## Come\u00e7ando\n\n[]()O primeiro passo \u00e9 recarregar os campos. Esse passo \u00e9 necess\u00e1rio para que o Kibana saiba que os campos do log do Apache est\u00e3o indexados e permita us\u00e1-los em buscas. Para isso, v\u00e1 em Settings -> Indexes. Na barra da esquerda voc\u00ea pode ver todos os \u00edndices do Elasticsearch que o Kibana vai carregar. Isso permite restringir o Kibana a apenas os \u00edndices que ser\u00e3o analisados.\u00a0O Kibana\u00a0j\u00e1 vem com o padr\u00e3o de nome dos \u00edndices gerados pelo Logstash por default. Se isso n\u00e3o acontecer, clique em \u201c*Add new*\u201d e crie um \u00edndice de pattern *logstash-*.*\n\n[![1-reindex](http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/1-reindex.png)](http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/1-reindex.png)\n\n\u00a0\n\nPara recarregar os campos, escolha o \u00edndice do logstash e pressione\u00a0o bot\u00e3o de *refresh* laranja no canto superior direito.\u00a0Os campos do log do Apache (path, request, agent etc) devem aparecer como \u201cindexed\u201d na lista de campos.\n\nMude para a aba \u201cDiscover\u201d. Aqui voc\u00ea deve ver um gr\u00e1fico com a quantidade de entradas por dia e as entradas do log Apache em texto puro. Se isso n\u00e3o acontecer, aumente\u00a0o per\u00edodo de an\u00e1lise no canto superior direito. Se os logs continuarem n\u00e3o aparecendo, verifique se eles est\u00e3o carregados corretamente no Elasticsearch com os seguintes comandos:\n\n`curl 'localhost:9200\/_cat\/indices?v'` \u2013 Lista todos os \u00edndices do Elasticsearch\n\n`curl -XGET 'localhost:9200\/<nome_do_indice>\/_search?'` \u2013 \u00a0Lista todas as entradas de um \u00edndice\n\nNa barra lateral esquerda, est\u00e3o listados todos os\u00a0campos do \u00edndice. Clicando neles voc\u00ea pode ver uma contagem de cada uma das suas ocorr\u00eancias. Voc\u00ea pode exibir apenas\u00a0alguns campos ao inv\u00e9s\u00a0da entrada em texto puro clicando em \u201c*add*\u201d nos campos desejados.\n\nSe voc\u00ea viu o erro\u00a0\u201cThis field is not indexed thus unavailable for visualization and search\u201d ou\u00a0\u201cunindexed fields cannot be searched\u201d ao selecionar os campos do Apache, voc\u00ea precisa [recarregar os campos](#refresh-fields).\n\n\n## Criando Visualiza\u00e7\u00f5es\n\nPara criar um gr\u00e1fico de barras verticais\u00a0com a\u00a0contagem de cada valor de um campo, basta selecion\u00e1-lo e clicar em \u201cVisualize\u201d:\n\n[![agent-chart](http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/agent-chart.png)](http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/agent-chart.png)\n\nSalve\u00a0essa visualiza\u00e7\u00e3o\u00a0clicando no bot\u00e3o save no canto superior direito.\n\nExistem outras formas de visualiza\u00e7\u00e3o de dados. Mude para a aba \u201c*Visualize*\u201c: Voc\u00ea vai ver uma\u00a0lista de\u00a0visualiza\u00e7\u00f5es e seus principais usos. Vamos criar uma outra visualiza\u00e7\u00e3o para contar a quantidade de erros 500. Escolha a visualiza\u00e7\u00e3o \u201cMetric\u201d e no passo seguinte, escolha \u201cFrom a new search\u201d.\n\nA sintaxe de busca do Kibana \u00e9 a mesma do Elasticsearch (que \u00e9 a mesma do Apache Lucene). A query\u00a0`response: 500`\u00a0retorna apenas os erros 500 do\u00a0log do Apache. Troque o * por essa query. Salve tamb\u00e9m essa visualiza\u00e7\u00e3o.\n\n\n## Criando um Dashboard\n\nMude para a aba Dashboard. Ao clicar no bot\u00e3o\u00a0**+** no canto superior direito, voc\u00ea pode adicionar as duas visualiza\u00e7\u00f5es que criamos. Elas podem ser redimensionadas e reorganizadas da maneira que voc\u00ea quiser.\n\n\u00a0\n\n[![Screen Shot 2015-08-25 at 10.56.34 AM](http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/Screen-Shot-2015-08-25-at-10.56.34-AM.png)](http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/Screen-Shot-2015-08-25-at-10.56.34-AM.png)\n\n\u00a0\n\n\u00a0\n\n[yasr_visitor_votes size=\u201dsmall\u201d]\n\n\n","html":"<p>No post\u00a0<a href=\"http:\/\/rafaelmt.github.io\/pt\/2015\/08\/18\/gerenciando-logs-com-o-stack-elk\/\">Gerenciando Logs com o Stack ELK<\/a>\u00a0instalamos e configuramos um Stack ELK para\u00a0consolidar e analizar logs de um Servidor Apache.\u00a0Nesse post vou falar um pouco mais do Kibana e como\u00a0us\u00e1-lo para\u00a0criar gr\u00e1ficos e filtros para esses dados.<\/p>\n<p><!--more--><\/p>\n<p>&nbsp;<\/p>\n<h2>Come\u00e7ando<\/h2>\n<p><a id=\"refresh-fields\"><\/a>O primeiro passo \u00e9 recarregar os campos. Esse passo \u00e9 necess\u00e1rio para que o Kibana saiba que os campos do log do Apache est\u00e3o indexados e permita us\u00e1-los em buscas. Para isso, v\u00e1 em Settings -&gt; Indexes. Na barra da esquerda voc\u00ea pode ver todos os \u00edndices do Elasticsearch que o Kibana vai carregar. Isso permite restringir o Kibana a apenas os \u00edndices que ser\u00e3o analisados.\u00a0O Kibana\u00a0j\u00e1 vem com o padr\u00e3o de nome dos \u00edndices gerados pelo Logstash por default. Se isso n\u00e3o acontecer, clique em &#8220;<em>Add new<\/em>&#8221; e crie um \u00edndice de pattern <em>logstash-*.<\/em><\/p>\n<p><a href=\"http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/1-reindex.png\"><img class=\"wp-image-105 aligncenter\" src=\"http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/1-reindex.png\" alt=\"1-reindex\" width=\"475\" height=\"201\" \/><\/a><\/p>\n<p>&nbsp;<\/p>\n<p>Para recarregar os campos, escolha o \u00edndice do logstash e pressione\u00a0o bot\u00e3o de <em>refresh<\/em> laranja no canto superior direito.\u00a0Os campos do log do Apache (path, request, agent etc) devem aparecer como &#8220;indexed&#8221; na lista de campos.<\/p>\n<p>Mude para a aba &#8220;Discover&#8221;. Aqui voc\u00ea deve ver um gr\u00e1fico com a quantidade de entradas por dia e as entradas do log Apache em texto puro. Se isso n\u00e3o acontecer, aumente\u00a0o per\u00edodo de an\u00e1lise no canto superior direito. Se os logs continuarem n\u00e3o aparecendo, verifique se eles est\u00e3o carregados corretamente no Elasticsearch com os seguintes comandos:<\/p>\n<p><code>curl 'localhost:9200\/_cat\/indices?v'<\/code> &#8211; Lista todos os \u00edndices do Elasticsearch<\/p>\n<p><code>curl -XGET 'localhost:9200\/&lt;nome_do_indice&gt;\/_search?'<\/code> &#8211; \u00a0Lista todas as entradas de um \u00edndice<\/p>\n<p>Na barra lateral esquerda, est\u00e3o listados todos os\u00a0campos do \u00edndice. Clicando neles voc\u00ea pode ver uma contagem de cada uma das suas ocorr\u00eancias. Voc\u00ea pode exibir apenas\u00a0alguns campos ao inv\u00e9s\u00a0da entrada em texto puro clicando em &#8220;<em>add<\/em>&#8221; nos campos desejados.<\/p>\n<p>Se voc\u00ea viu o erro\u00a0&#8220;This field is not indexed thus unavailable for visualization and search&#8221; ou\u00a0&#8220;unindexed fields cannot be searched&#8221; ao selecionar os campos do Apache, voc\u00ea precisa <a href=\"#refresh-fields\">recarregar os campos<\/a>.<\/p>\n<h2>Criando Visualiza\u00e7\u00f5es<\/h2>\n<p>Para criar um gr\u00e1fico de barras verticais\u00a0com a\u00a0contagem de cada valor de um campo, basta selecion\u00e1-lo e clicar em &#8220;Visualize&#8221;:<\/p>\n<p><a href=\"http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/agent-chart.png\"><img class=\" wp-image-110 alignnone\" src=\"http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/agent-chart.png\" alt=\"agent-chart\" width=\"695\" height=\"341\" \/><\/a><\/p>\n<p>Salve\u00a0essa visualiza\u00e7\u00e3o\u00a0clicando no bot\u00e3o save no canto superior direito.<\/p>\n<p>Existem outras formas de visualiza\u00e7\u00e3o de dados. Mude para a aba &#8220;<em>Visualize<\/em>&#8220;: Voc\u00ea vai ver uma\u00a0lista de\u00a0visualiza\u00e7\u00f5es e seus principais usos. Vamos criar uma outra visualiza\u00e7\u00e3o para contar a quantidade de erros 500. Escolha a visualiza\u00e7\u00e3o &#8220;Metric&#8221; e no passo seguinte, escolha &#8220;From a new search&#8221;.<\/p>\n<p>A sintaxe de busca do Kibana \u00e9 a mesma do Elasticsearch (que \u00e9 a mesma do Apache Lucene). A query\u00a0<code>response: 500<\/code>\u00a0retorna apenas os erros 500 do\u00a0log do Apache. Troque o * por essa query. Salve tamb\u00e9m essa visualiza\u00e7\u00e3o.<\/p>\n<h2>Criando um Dashboard<\/h2>\n<p>Mude para a aba Dashboard. Ao clicar no bot\u00e3o\u00a0<strong>+<\/strong> no canto superior direito, voc\u00ea pode adicionar as duas visualiza\u00e7\u00f5es que criamos. Elas podem ser redimensionadas e reorganizadas da maneira que voc\u00ea quiser.<\/p>\n<p>&nbsp;<\/p>\n<p><a href=\"http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/Screen-Shot-2015-08-25-at-10.56.34-AM.png\"><img class=\"wp-image-112 alignnone\" src=\"http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/Screen-Shot-2015-08-25-at-10.56.34-AM.png\" alt=\"Screen Shot 2015-08-25 at 10.56.34 AM\" width=\"727\" height=\"356\" \/><\/a><\/p>\n<p>&nbsp;<\/p>\n<p>&nbsp;<\/p>\n<p>[yasr_visitor_votes size=&#8221;small&#8221;]<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 25 Aug 2015 11:43:01 +0000","created_by":1,"updated_at":"Fri, 04 Sep 2015 15:13:30 +0000","updated_by":1,"published_at":"Tue, 25 Aug 2015 11:43:01 +0000","published_by":1},{"id":132,"title":"Tutorial Datomic","slug":"temp-slug-11","markdown":"","html":"","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Fri, 28 Aug 2015 10:45:41 +0000","created_by":1,"updated_at":"Fri, 28 Aug 2015 10:45:41 +0000","updated_by":1,"published_at":"","published_by":1},{"id":128,"title":"Desligando Inst\u00e2ncias AWS Linux Automaticamente","slug":"desligando-instancias-aws-linux-automaticamente","markdown":"\nConfesse, voc\u00ea j\u00e1 esqueceu de desligar uma inst\u00e2ncia EC2 na\u00a0AWS que voc\u00ea criou s\u00f3 pra fazer um teste r\u00e1pido. Dependendo do tamanho da inst\u00e2ncia e de quanto tempo voc\u00ea demorou para perceber, esse erro pode sair caro. E se a sua mem\u00f3ria \u00e9 t\u00e3o ruim quanto a minha, voc\u00ea j\u00e1 vez isso mais de uma vez \u00a0\u2026\n\nSeus problemas acabaram!**\u2122**\u00a0Para n\u00e3o fazer isso de novo, eu criei um\u00a0script que desliga a m\u00e1quina caso n\u00e3o haja nenhuma sess\u00e3o SSH ap\u00f3s uma quantidade de minutos:\n\n<style>.gist table { margin-bottom: 0; }<\/style><div class=\"gist-oembed\" data-gist=\"20a89197200f95d30b38.json\"><\/div>O tempo \u00e9 passado em minutos como par\u00e2metro. N\u00e3o testei em todas as vers\u00f5es, mas deve funcionar em distros baseadas em RHEL (Centos, Amazon Linux etc). Basta coloc\u00e1-lo na crontab do usu\u00e1rio root para executar com uma frequ\u00eancia menor que o limite de tempo desejado. Para checar a cada minuto e desligar ap\u00f3s 30 minutos sem sess\u00e3o SSH, por exemplo, adicione a seguinte linha \u00e0 crontab do root (`sudo crontab -e`):\n\n`*\/1 * * * * \/home\/centos\/autoshutdown.sh 30`\n\nTroque o caminho para o local onde voc\u00ea salvou o script.\n\nS\u00f3 n\u00e3o v\u00e1 esquecer de remover quando for para produ\u00e7\u00e3o!\n\n[yasr_visitor_votes size=\u201dsmall\u201d]\n\n\n","html":"<p>Confesse, voc\u00ea j\u00e1 esqueceu de desligar uma inst\u00e2ncia EC2 na\u00a0AWS que voc\u00ea criou s\u00f3 pra fazer um teste r\u00e1pido. Dependendo do tamanho da inst\u00e2ncia e de quanto tempo voc\u00ea demorou para perceber, esse erro pode sair caro. E se a sua mem\u00f3ria \u00e9 t\u00e3o ruim quanto a minha, voc\u00ea j\u00e1 vez isso mais de uma vez \u00a0&#8230;<\/p>\n<p><!--more--><\/p>\n<p>Seus problemas acabaram!<b>\u2122<\/b>\u00a0Para n\u00e3o fazer isso de novo, eu criei um\u00a0script que desliga a m\u00e1quina caso n\u00e3o haja nenhuma sess\u00e3o SSH ap\u00f3s uma quantidade de minutos:<\/p>\n<style>.gist table { margin-bottom: 0; }<\/style><div class=\"gist-oembed\" data-gist=\"20a89197200f95d30b38.json\"><\/div>\n<p>O tempo \u00e9 passado em minutos como par\u00e2metro. N\u00e3o testei em todas as vers\u00f5es, mas deve funcionar em distros baseadas em RHEL (Centos, Amazon Linux etc). Basta coloc\u00e1-lo na crontab do usu\u00e1rio root para executar com uma frequ\u00eancia menor que o limite de tempo desejado. Para checar a cada minuto e desligar ap\u00f3s 30 minutos sem sess\u00e3o SSH, por exemplo, adicione a seguinte linha \u00e0 crontab do root (<code>sudo crontab -e<\/code>):<\/p>\n<p><code>*\/1 * * * * \/home\/centos\/autoshutdown.sh 30<\/code><\/p>\n<p>Troque o caminho para o local onde voc\u00ea salvou o script.<\/p>\n<p>S\u00f3 n\u00e3o v\u00e1 esquecer de remover quando for para produ\u00e7\u00e3o!<\/p>\n<p>[yasr_visitor_votes size=&#8221;small&#8221;]<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Sat, 29 Aug 2015 14:55:38 +0000","created_by":1,"updated_at":"Fri, 04 Sep 2015 15:13:22 +0000","updated_by":1,"published_at":"Sat, 29 Aug 2015 14:55:38 +0000","published_by":1},{"id":124,"title":"Kibana Tutorial","slug":"kibana-tutorial","markdown":"\nIn the\u00a0[Managing Logs with the ELK Stack](http:\/\/rafaelmt.github.io\/en\/2015\/08\/19\/managing-logs-with-the-elk-stack\/)\u00a0post, we have installed and configured an ELK stack to consolidate and analyze logs from an Apache Server. In this post I\u2019ll talk a little more about Kibana and how to use it to create data charts and filter data.\n\n\n## Getting Started\n\n[]()First step is reloading the fields. This step is necessary so Kibana knows that the Apache log fields are indexed and can be used for searches. To do so, go to Settings -> Indexes. In the left-hand bar you\u2019ll see all Elasticsearch indexes Kibana will load. This allows us to restrict Kibana\u2019s access to the analyzed indexes only. Kibana comes with the Logstash index\u2019s name pattern by default. If it does not happen, click \u201c*Add new\u201d* and create an index with\u00a0*logstash-*\u00a0*as pattern.\n\n[![1-reindex](http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/1-reindex.png)](http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/1-reindex.png)\n\nTo reload the fields, choose Logstash index and press the orange\u00a0*refresh* button on the top right corner. The Apache log fields (path, request, agent etc.) should be displayed as\u00a0*indexed* in the field list.\n\nLet\u2019s switch to the \u201cDiscover\u201d tab. You will see a chart containing the amount of entries per day and the apache log entries in unparsed text. If you don\u2019t see those, increase the analysis period on the top right corner. If logs still\u00a0don\u2019t appear, check if they are correctly loaded in Elasticsearch with the following commands:\n\n`curl 'localhost:9200\/_cat\/indices?v'` \u2013\u00a0List all Elasticsearch indexes\n\n`curl -XGET 'localhost:9200\/<nome_do_indice>\/_search?'` \u2013 \u00a0List all entries in an index.\n\nThe index\u2019 fields are listed in the left-hand tab. Clicking them will show a count of occurrences. You can\u00a0display the selected fields only instead of log text by clicking \u201c*add*\u201c.\n\nIf you see\u00a0\u201cThis field is not indexed thus unavailable for visualization and search\u201d ou\u00a0\u201cunindexed fields cannot be searched\u201d when trying to add the Apache fields you need to [reload the fields](#refresh-fields)\n\n\n## Creating Visualizations\n\nTo create a vertical bar graph containing the count of every value in a field, select it and click \u201cVisualize\u201d:\n\n[![agent-chart](http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/agent-chart.png)](http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/agent-chart.png)\n\nSave the visualization by clicking the top right corner *save* button.\n\nThere are other forms of data visualization. In the *Visualize* you will see a list of possible visualizations and its use cases. Let\u2019s create another visualization to count the errors 500 returned by the server. Select the \u201cMetric\u201d visualization and in the next step choose \u201cFrom a new search\u201d.\n\nKibana\u2019s search syntax is the same as Elasticsearch\u2019s (with is the same as Apache Lucene\u2019s). The\u00a0`response: 500`\u00a0query returns the responses\u00a0with code 500. Replace * for this query. Save it in another visualization.\n\n\n## Creating a Dashboard\n\nChange to the Dashboard tab. By clicking + in the top right corner, you can add visualizations to your Dashboard. They can be re-dimensioned and re-organized.\n\n[![Screen Shot 2015-08-25 at 10.56.34 AM](http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/Screen-Shot-2015-08-25-at-10.56.34-AM.png)](http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/Screen-Shot-2015-08-25-at-10.56.34-AM.png)\n\n\u00a0\n\n[yasr_visitor_votes size=\u201dsmall\u201d]\n\n\n","html":"<p>In the\u00a0<a href=\"http:\/\/rafaelmt.github.io\/en\/2015\/08\/19\/managing-logs-with-the-elk-stack\/\">Managing Logs with the ELK Stack<\/a>\u00a0post, we have installed and configured an ELK stack to consolidate and analyze logs from an Apache Server. In this post I&#8217;ll talk a little more about Kibana and how to use it to create data charts and filter data.<\/p>\n<p><!--more--><\/p>\n<h2>Getting Started<\/h2>\n<p><a id=\"refresh-fields\"><\/a>First step is reloading the fields. This step is necessary so Kibana knows that the Apache log fields are indexed and can be used for searches. To do so, go to Settings -&gt; Indexes. In the left-hand bar you&#8217;ll see all Elasticsearch indexes Kibana will load. This allows us to restrict Kibana&#8217;s access to the analyzed indexes only. Kibana comes with the Logstash index&#8217;s name pattern by default. If it does not happen, click &#8220;<em>Add new&#8221;<\/em> and create an index with\u00a0<em>logstash-*\u00a0<\/em>as pattern.<\/p>\n<p><a href=\"http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/1-reindex.png\"><img class=\"wp-image-105 aligncenter\" src=\"http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/1-reindex.png\" alt=\"1-reindex\" width=\"475\" height=\"201\" \/><\/a><\/p>\n<p>To reload the fields, choose Logstash index and press the orange\u00a0<em>refresh<\/em> button on the top right corner. The Apache log fields (path, request, agent etc.) should be displayed as\u00a0<em>indexed<\/em> in the field list.<\/p>\n<p>Let&#8217;s switch to the &#8220;Discover&#8221; tab. You will see a chart containing the amount of entries per day and the apache log entries in unparsed text. If you don&#8217;t see those, increase the analysis period on the top right corner. If logs still\u00a0don&#8217;t appear, check if they are correctly loaded in Elasticsearch with the following commands:<\/p>\n<p><code>curl 'localhost:9200\/_cat\/indices?v'<\/code> &#8211;\u00a0List all Elasticsearch indexes<\/p>\n<p><code>curl -XGET 'localhost:9200\/&lt;nome_do_indice&gt;\/_search?'<\/code> &#8211; \u00a0List all entries in an index.<\/p>\n<p>The index&#8217; fields are listed in the left-hand tab. Clicking them will show a count of occurrences. You can\u00a0display the selected fields only instead of log text by clicking &#8220;<em>add<\/em>&#8220;.<\/p>\n<p>If you see\u00a0&#8220;This field is not indexed thus unavailable for visualization and search&#8221; ou\u00a0&#8220;unindexed fields cannot be searched&#8221; when trying to add the Apache fields you need to <a href=\"#refresh-fields\">reload the fields<\/a><\/p>\n<h2>Creating Visualizations<\/h2>\n<p>To create a vertical bar graph containing the count of every value in a field, select it and click &#8220;Visualize&#8221;:<\/p>\n<p><a href=\"http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/agent-chart.png\"><img class=\" wp-image-110 alignnone\" src=\"http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/agent-chart.png\" alt=\"agent-chart\" width=\"695\" height=\"341\" \/><\/a><\/p>\n<p>Save the visualization by clicking the top right corner <em>save<\/em> button.<\/p>\n<p>There are other forms of data visualization. In the <em>Visualize<\/em> you will see a list of possible visualizations and its use cases. Let&#8217;s create another visualization to count the errors 500 returned by the server. Select the &#8220;Metric&#8221; visualization and in the next step choose &#8220;From a new search&#8221;.<\/p>\n<p>Kibana&#8217;s search syntax is the same as Elasticsearch&#8217;s (with is the same as Apache Lucene&#8217;s). The\u00a0<code>response: 500<\/code>\u00a0query returns the responses\u00a0with code 500. Replace * for this query. Save it in another visualization.<\/p>\n<h2>Creating a Dashboard<\/h2>\n<p>Change to the Dashboard tab. By clicking + in the top right corner, you can add visualizations to your Dashboard. They can be re-dimensioned and re-organized.<\/p>\n<p><a href=\"http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/Screen-Shot-2015-08-25-at-10.56.34-AM.png\"><img class=\"wp-image-112 alignnone\" src=\"http:\/\/rafaelmt.github.io\/wp-content\/uploads\/2015\/08\/Screen-Shot-2015-08-25-at-10.56.34-AM.png\" alt=\"Screen Shot 2015-08-25 at 10.56.34 AM\" width=\"727\" height=\"356\" \/><\/a><\/p>\n<p>&nbsp;<\/p>\n<p>[yasr_visitor_votes size=&#8221;small&#8221;]<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 01 Sep 2015 00:30:20 +0000","created_by":1,"updated_at":"Fri, 04 Sep 2015 15:13:16 +0000","updated_by":1,"published_at":"Tue, 01 Sep 2015 00:30:20 +0000","published_by":1},{"id":140,"title":"Automatically Shutting Down Idle AWS Linux Instances","slug":"automatically-shutting-down-aws-linux-instances","markdown":"\nLet me guess: you forgot to turn off an EC2 instance that you\u2019ve created to run a quick test only. Depending on how big the instance is and how long it took you to realize, it can be a costly mistake. And if you memory is as bad as mine, you have been there a couple of times \u2026\n\nTo solve this problem, I created a script that\u00a0turns off the machine after a given time without any SSH sessions:\n\n<style>.gist table { margin-bottom: 0; }<\/style><div class=\"gist-oembed\" data-gist=\"20a89197200f95d30b38.json\"><\/div>The time is passed as a parameter (in minutes). I haven\u2019t tested in all distros, but it should be fine with RHEL-based ones (Centos, Amazon Linux etc). Just put it in the root\u2019s crontab with a higher frequency than the desired timeout.. To check every minute and shut down after 30 minutes, for example, add the following line to root\u2019s crontab (`sudo crontab -e`):\n\n`*\/1 * * * * \/home\/centos\/autoshutdown.sh 30`\n\nChange the path to where the script is saved. Also make sure you have the permission\u00a0to execute it\n\nAnd please don\u2019t forget to remove it before going to production!\n\n\u00a0\n\n[yasr_visitor_votes size=\u201dsmall\u201d]\n\n\n","html":"<p>Let me guess: you forgot to turn off an EC2 instance that you&#8217;ve created to run a quick test only. Depending on how big the instance is and how long it took you to realize, it can be a costly mistake. And if you memory is as bad as mine, you have been there a couple of times &#8230;<\/p>\n<p><!--more--><\/p>\n<p>To solve this problem, I created a script that\u00a0turns off the machine after a given time without any SSH sessions:<\/p>\n<style>.gist table { margin-bottom: 0; }<\/style><div class=\"gist-oembed\" data-gist=\"20a89197200f95d30b38.json\"><\/div>\n<p>The time is passed as a parameter (in minutes). I haven&#8217;t tested in all distros, but it should be fine with RHEL-based ones (Centos, Amazon Linux etc). Just put it in the root&#8217;s crontab with a higher frequency than the desired timeout.. To check every minute and shut down after 30 minutes, for example, add the following line to root&#8217;s crontab (<code>sudo crontab -e<\/code>):<\/p>\n<p><code>*\/1 * * * * \/home\/centos\/autoshutdown.sh 30<\/code><\/p>\n<p>Change the path to where the script is saved. Also make sure you have the permission\u00a0to execute it<\/p>\n<p>And please don&#8217;t forget to remove it before going to production!<\/p>\n<p>&nbsp;<\/p>\n<p>[yasr_visitor_votes size=&#8221;small&#8221;]<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 01 Sep 2015 00:49:09 +0000","created_by":1,"updated_at":"Fri, 04 Dec 2015 10:58:22 +0000","updated_by":1,"published_at":"Tue, 01 Sep 2015 00:49:09 +0000","published_by":1},{"id":145,"title":"\u00c9 poss\u00edvel ser PCI-compliant com o stack ELK?","slug":"temp-slug-15","markdown":"\nTL;DR: \u00a0n\u00e3o. O stack ELK n\u00e3o atende todos os requisitos do PCI DSS para logs.\n\nRequirement 10\n\n\n","html":"<p>TL;DR: \u00a0n\u00e3o. O stack ELK n\u00e3o atende todos os requisitos do PCI DSS para logs.<\/p>\n<p>Requirement 10<\/p>\n","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 01 Sep 2015 10:12:41 +0000","created_by":1,"updated_at":"Tue, 01 Sep 2015 10:12:41 +0000","updated_by":1,"published_at":"","published_by":1},{"id":5,"title":"Mandando Emails usando SES","slug":"mandando-emails-usando-ses","markdown":"","html":"","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 01 Sep 2015 10:29:41 +0000","created_by":1,"updated_at":"Tue, 01 Sep 2015 10:30:15 +0000","updated_by":1,"published_at":"","published_by":1},{"id":147,"title":"Tutorial - Criando um Paint compartilhado usando WebRTC","slug":"temp-slug-17","markdown":"","html":"","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 01 Sep 2015 15:10:39 +0000","created_by":1,"updated_at":"Tue, 01 Sep 2015 15:10:39 +0000","updated_by":1,"published_at":"","published_by":1},{"id":166,"title":"Cortando custos com Spot Instances na AWS","slug":"temp-slug-18","markdown":"","html":"","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 08 Sep 2015 10:55:32 +0000","created_by":1,"updated_at":"Tue, 08 Sep 2015 10:55:32 +0000","updated_by":1,"published_at":"","published_by":1},{"id":149,"title":"Is ELK PCI-Compliant out-of-the-box?","slug":"temp-slug-19","markdown":"\nIn the\u00a0[Managing Logs with the ELK Stack](http:\/\/rafaelmt.github.io\/en\/2015\/08\/19\/managing-logs-with-the-elk-stack\/)\u00a0post I\u2019ve shown how you can\u00a0setup a log manager using the ELK stack. One of the\u00a0[PCI DSS guidelines](https:\/\/www.pcisecuritystandards.org\/documents\/PCI_DSS_v3-1.pdf)\u00a0is having a log manager that consolidates and keeps a backup of all\u00a0audit trails generated by your systems. So the question is: is it possible to use an out-of-the-box ELK stack to plug this hole in your\u00a0PCI-compliant environment?\n\nThe short answer is no. There are shortcomings on both Kibana and Logstash that prevents you from fulfilling the PCI requirements with an out-of-the-box ELK installation. But let\u2019s go through each of these shortcomings\u00a0and see how you\u00a0can address them. You can see all the audit trail related requirements\u00a0in the item\u00a010 of the \u201cRequirements and Security Assessment Procedures\u201d document, linked above.\n\n#### 10.2.3 Access to all audit trails\n\nThis requirement says you need to keep track of who\u00a0accesses the logs (logception), to keep unauthorized people from tampering them or using them for\u00a0wrongdoing. And\u00a0that\u2019s\u00a0where ELK falls short. As per the current release (4.1.1), Kibana does not have Authorization or Authentication, let alone keeping an access log.\n\nOne possible solution\u00a0would be using\u00a0another web server in front of Kibana that adds these features (authorization, authentication and logging). A simple Apache server that simply relays all requests to Kibana after\u00a0authentication would solve the issue.\n\n### 10.5.1 Limit viewing of audit trails to those with a job-related need\n\nThe lack of Authorization in Kibana makes it impossible to limit\u00a0viewing of logs to people who need it.\u00a0A possible solution would be having multiple Kibana instances, each having access to a specific set of Elasticsearch indexes. However, since there\u2019s no authorization of any kind, any user with access to\u00a0Kibana can tamper with the indexes, allowing a curious developer to end up with access to the firewall logs, for example.\n\n#### Conclusion\n\nWhile there might be workarounds to make an\u00a0ELK stack PCI-ready, such as adding an Apache server for Authentication\/Authorization and using multiple Kibana instances\u00a0to create multiple audit trails, I\u2019ve never tried anything like this myself, and\u00a0it certainly looks cumbersome. I would recommend keeping\u00a0ELK stack usage to\u00a0systems where PCI-compliance is not a concern.\n\n#### The Future\n\nThe community around ELK\u00a0is aware of these shortcomings and is willing to address them. There are open discussions about [authentication](https:\/\/github.com\/elastic\/kibana\/issues\/3904)\u00a0and\u00a0[authorization](https:\/\/github.com\/elastic\/kibana\/issues\/4453)\u00a0in the Kibana\u2019s Github. If you are\u00a0interested in these features, make sure you chime-in into these\u00a0discussions and let them know what you use-case is.\n\n### \n\n\n","html":"<p>In the\u00a0<a href=\"http:\/\/rafaelmt.github.io\/en\/2015\/08\/19\/managing-logs-with-the-elk-stack\/\">Managing Logs with the ELK Stack<\/a>\u00a0post I&#8217;ve shown how you can\u00a0setup a log manager using the ELK stack. One of the\u00a0<a href=\"https:\/\/www.pcisecuritystandards.org\/documents\/PCI_DSS_v3-1.pdf\">PCI DSS guidelines<\/a>\u00a0is having a log manager that consolidates and keeps a backup of all\u00a0audit trails generated by your systems. So the question is: is it possible to use an out-of-the-box ELK stack to plug this hole in your\u00a0PCI-compliant environment?<\/p>\n<p>The short answer is no. There are shortcomings on both Kibana and Logstash that prevents you from fulfilling the PCI requirements with an out-of-the-box ELK installation. But let&#8217;s go through each of these shortcomings\u00a0and see how you\u00a0can address them. You can see all the audit trail related requirements\u00a0in the item\u00a010 of the &#8220;Requirements and Security Assessment Procedures&#8221; document, linked above.<\/p>\n<h4>10.2.3 Access to all audit trails<\/h4>\n<p>This requirement says you need to keep track of who\u00a0accesses the logs (logception), to keep unauthorized people from tampering them or using them for\u00a0wrongdoing. And\u00a0that&#8217;s\u00a0where ELK falls short. As per the current release (4.1.1), Kibana does not have Authorization or Authentication, let alone keeping an access log.<\/p>\n<p>One possible solution\u00a0would be using\u00a0another web server in front of Kibana that adds these features (authorization, authentication and logging). A simple Apache server that simply relays all requests to Kibana after\u00a0authentication would solve the issue.<\/p>\n<h3>10.5.1 Limit viewing of audit trails to those with a job-related need<\/h3>\n<p>The lack of Authorization in Kibana makes it impossible to limit\u00a0viewing of logs to people who need it.\u00a0A possible solution would be having multiple Kibana instances, each having access to a specific set of Elasticsearch indexes. However, since there&#8217;s no authorization of any kind, any user with access to\u00a0Kibana can tamper with the indexes, allowing a curious developer to end up with access to the firewall logs, for example.<\/p>\n<h4>Conclusion<\/h4>\n<p>While there might be workarounds to make an\u00a0ELK stack PCI-ready, such as adding an Apache server for Authentication\/Authorization and using multiple Kibana instances\u00a0to create multiple audit trails, I&#8217;ve never tried anything like this myself, and\u00a0it certainly looks cumbersome. I would recommend keeping\u00a0ELK stack usage to\u00a0systems where PCI-compliance is not a concern.<\/p>\n<h4>The Future<\/h4>\n<p>The community around ELK\u00a0is aware of these shortcomings and is willing to address them. There are open discussions about <a href=\"https:\/\/github.com\/elastic\/kibana\/issues\/3904\">authentication<\/a>\u00a0and\u00a0<a href=\"https:\/\/github.com\/elastic\/kibana\/issues\/4453\">authorization<\/a>\u00a0in the Kibana&#8217;s Github. If you are\u00a0interested in these features, make sure you chime-in into these\u00a0discussions and let them know what you use-case is.<\/p>\n<h3><\/h3>\n","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 08 Sep 2015 13:34:47 +0000","created_by":1,"updated_at":"Tue, 08 Sep 2015 13:34:47 +0000","updated_by":1,"published_at":"","published_by":1},{"id":169,"title":"Adding password to Apache","slug":"temp-slug-20","markdown":"\nhttp:\/\/www.yolinux.com\/TUTORIALS\/LinuxTutorialApacheAddingLoginSiteProtection.html\n\n\n","html":"<p>http:\/\/www.yolinux.com\/TUTORIALS\/LinuxTutorialApacheAddingLoginSiteProtection.html<\/p>\n","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Wed, 09 Sep 2015 16:34:42 +0000","created_by":1,"updated_at":"Wed, 09 Sep 2015 16:34:42 +0000","updated_by":1,"published_at":"","published_by":1},{"id":171,"title":"Username password in kibana","slug":"temp-slug-21","markdown":"\nhttp:\/\/passportjs.org\/docs\/username-password\n\n\u00a0\n\nhttp:\/\/nginx.org\/en\/docs\/http\/ngx_http_auth_basic_module.html\n\n\u00a0\n\nhttp:\/\/www.yolinux.com\/TUTORIALS\/LinuxTutorialApacheAddingLoginSiteProtection.html#LDAP\n\n\n","html":"<p>http:\/\/passportjs.org\/docs\/username-password<\/p>\n<p>&nbsp;<\/p>\n<p>http:\/\/nginx.org\/en\/docs\/http\/ngx_http_auth_basic_module.html<\/p>\n<p>&nbsp;<\/p>\n<p>http:\/\/www.yolinux.com\/TUTORIALS\/LinuxTutorialApacheAddingLoginSiteProtection.html#LDAP<\/p>\n","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Wed, 09 Sep 2015 17:20:37 +0000","created_by":1,"updated_at":"Wed, 09 Sep 2015 17:20:37 +0000","updated_by":1,"published_at":"","published_by":1},{"id":55,"title":"Criando um pattern para Grok","slug":"temp-slug-22","markdown":"\nNo post\u00a0[Gerenciando Logs com o Stack ELK](http:\/\/rafaelmt.github.io\/pt\/2015\/08\/18\/gerenciando-logs-com-o-stack-elk\/)\u00a0usamos um filtro Grok para parsear o log do Apache e envi\u00e1-lo para ser indexado no ElasticSearch. Essa tarefa foi facilitada pelo fato de j\u00e1 existir um *pattern* para o log do Apache.\u00a0Nesse post veremos como criar um *pattern *para qualquer tipo de entrada.\n\n\n## Sintaxe\n\nA sintaxe de um pattern \u00e9 ` %{SYNTAX:SEMANTIC}`, onde:\n\n- **SYNTAX:** Formato do campo. Ex: TEXT para texto, NUMBER para n\u00fameros etc.\n- **SEMANTIC:** Nome do campo. Ex: client para o nome do cliente, ip para ip etc.\n\nEsses patterns s\u00e3o combinados de modo a montar o pattern desejado. Vamos dar uma olhada no padr\u00e3o do log do Apache para entender melhor:\n\n\u00a0\n\n\n","html":"<p>No post\u00a0<a href=\"http:\/\/rafaelmt.github.io\/pt\/2015\/08\/18\/gerenciando-logs-com-o-stack-elk\/\">Gerenciando Logs com o Stack ELK<\/a>\u00a0usamos um filtro Grok para parsear o log do Apache e envi\u00e1-lo para ser indexado no ElasticSearch. Essa tarefa foi facilitada pelo fato de j\u00e1 existir um <em>pattern<\/em> para o log do Apache.\u00a0Nesse post veremos como criar um <em>pattern <\/em>para qualquer tipo de entrada.<\/p>\n<h2>Sintaxe<\/h2>\n<p>A sintaxe de um pattern \u00e9 <code> %{SYNTAX:SEMANTIC}<\/code>, onde:<\/p>\n<ul>\n<li><strong>SYNTAX:<\/strong> Formato do campo. Ex: TEXT para texto, NUMBER para n\u00fameros etc.<\/li>\n<li><strong>SEMANTIC:<\/strong> Nome do campo. Ex: client para o nome do cliente, ip para ip etc.<\/li>\n<\/ul>\n<p>Esses patterns s\u00e3o combinados de modo a montar o pattern desejado. Vamos dar uma olhada no padr\u00e3o do log do Apache para entender melhor:<\/p>\n<p>&nbsp;<\/p>\n","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 15 Sep 2015 22:45:17 +0000","created_by":1,"updated_at":"Tue, 15 Sep 2015 22:45:17 +0000","updated_by":1,"published_at":"","published_by":1},{"id":177,"title":"Having Multiple Databases under a Single Instance in RDS","slug":"having-multiple-databases-under-a-single-instance-in-rds","markdown":"\nA not-so-well documented feature in AWS RDS is the ability of creating multiple databases under a single database instance. This can be a low-cost solution for test environments, since RDS charges per instance.\u00a0You can lump together multiple test databases into a single instance and save some bucks, for example.\n\nWhy not production? All instance resources will be shared among the databases,\u00a0causing one database to affect the performance of the others. Another issue would be the\u00a0database parameter group, which would have to be the same for all databases under the instance. All RDS backups are also instance-level, so you would be out of luck if you\u00a0needed a backup for one of the databases only. The AWS dashboard\u00a0does not seem to support this either, so you won\u2019t see a list of the databases under the instance.\n\n### How To\n\nThere\u2019s no way of creating other databases from the AWS\u00a0dashboard or even from the API. You will have to rely on the DB engine\u2019s commands to do it. For PostgreSQL, for example, connect to the instance (psql -h <RDS_endpoint> -U <master_user>) and run the following command:\n\n`CREATE DATABASE <new_db_name>;`\n\nFrom now on, you can connect to this DB with the regular psql command. To list the databases under the current instance, run `\\list` (or `\\l`). To switch databases, use `\\connect <database_name>`.\n\n\n","html":"<p>A not-so-well documented feature in AWS RDS is the ability of creating multiple databases under a single database instance. This can be a low-cost solution for test environments, since RDS charges per instance.\u00a0You can lump together multiple test databases into a single instance and save some bucks, for example.<\/p>\n<p>Why not production? All instance resources will be shared among the databases,\u00a0causing one database to affect the performance of the others. Another issue would be the\u00a0database parameter group, which would have to be the same for all databases under the instance. All RDS backups are also instance-level, so you would be out of luck if you\u00a0needed a backup for one of the databases only. The AWS dashboard\u00a0does not seem to support this either, so you won&#8217;t see a list of the databases under the instance.<\/p>\n<h3>How To<\/h3>\n<p>There&#8217;s no way of creating other databases from the AWS\u00a0dashboard or even from the API. You will have to rely on the DB engine&#8217;s commands to do it. For PostgreSQL, for example, connect to the instance (psql -h &lt;RDS_endpoint&gt; -U &lt;master_user&gt;) and run the following command:<\/p>\n<p><code>CREATE DATABASE &lt;new_db_name&gt;;<\/code><\/p>\n<p>From now on, you can connect to this DB with the regular psql command. To list the databases under the current instance, run <code>\\list<\/code> (or <code>\\l<\/code>). To switch databases, use <code>\\connect &lt;database_name&gt;<\/code>.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Mon, 28 Sep 2015 18:07:42 +0000","created_by":1,"updated_at":"Tue, 27 Sep 2016 18:27:30 +0000","updated_by":1,"published_at":"Mon, 28 Sep 2015 18:07:42 +0000","published_by":1},{"id":180,"title":"M\u00faltiplos Bancos de Dados sob uma Inst\u00e2ncia no RDS","slug":"multiplos-bancos-de-dados-sob-uma-instancia-no-rds","markdown":"\nUma feature pouco documentada do\u00a0Relational Database Services da AWS \u00e9 a possibilidade de criar diversos bancos de dados sob uma mesma inst\u00e2ncia. Pode ser uma solu\u00e7\u00e3o interessante e de baixo custo para ambientes de testes, j\u00e1 que o uso do servi\u00e7o \u00e9 cobrado por inst\u00e2ncia. Voc\u00ea pode juntar alguns bancos de dados do seu ambiente de testes em uma inst\u00e2ncia e economizar um pouco na conta.\n\nE por que n\u00e3o usar em produ\u00e7\u00e3o? Todos os recursos da inst\u00e2ncia ser\u00e3o divididos entre os bancos, permitindo que um banco afete na performance dos outros. Outro problema \u00e9 o *parameter group*, que teria de ser o mesmo para todos os bancos de dados. Os backups tamb\u00e9m s\u00e3o feitos em toda a inst\u00e2ncia, portanto voc\u00ea teria problemas se\u00a0precisasse reverter apenas um dos bancos.\n\n### Como Fazer\n\nN\u00e3o h\u00e1 maneira de\u00a0criar outros bancos de dados usando o\u00a0dashboard do AWS ou pela API. Para isso, voc\u00ea vai ter de usar comandos da Engine de banco de dados rodando por baixo do RDS. No caso do\u00a0PostgreSQL, por exemplo, conecte-se \u00e0 inst\u00e2ncia (psql -h <RDS_endpoint> -U <master_user>) e rode o seguinte comando:\n\n`CREATE DATABASE <new_db_name>;`\n\nA partir de agora, voc\u00ea pode se conectar a esse banco de dados e us\u00e1-lo normalmente. Se voc\u00ea\u00a0configurou backups autom\u00e1ticos para essa inst\u00e2ncia, os backups cont\u00e9m todos os bancos de dados. Para listar os bancos de dados sob a inst\u00e2ncia, rode \u00a0`\\list` (ou\u00a0`\\l`). Para se conectar a outro banco, use o comando\u00a0`\\connect <database_name>`.\n\n\n","html":"<p>Uma feature pouco documentada do\u00a0Relational Database Services da AWS \u00e9 a possibilidade de criar diversos bancos de dados sob uma mesma inst\u00e2ncia. Pode ser uma solu\u00e7\u00e3o interessante e de baixo custo para ambientes de testes, j\u00e1 que o uso do servi\u00e7o \u00e9 cobrado por inst\u00e2ncia. Voc\u00ea pode juntar alguns bancos de dados do seu ambiente de testes em uma inst\u00e2ncia e economizar um pouco na conta.<\/p>\n<p>E por que n\u00e3o usar em produ\u00e7\u00e3o? Todos os recursos da inst\u00e2ncia ser\u00e3o divididos entre os bancos, permitindo que um banco afete na performance dos outros. Outro problema \u00e9 o <em>parameter group<\/em>, que teria de ser o mesmo para todos os bancos de dados. Os backups tamb\u00e9m s\u00e3o feitos em toda a inst\u00e2ncia, portanto voc\u00ea teria problemas se\u00a0precisasse reverter apenas um dos bancos.<\/p>\n<h3>Como Fazer<\/h3>\n<p>N\u00e3o h\u00e1 maneira de\u00a0criar outros bancos de dados usando o\u00a0dashboard do AWS ou pela API. Para isso, voc\u00ea vai ter de usar comandos da Engine de banco de dados rodando por baixo do RDS. No caso do\u00a0PostgreSQL, por exemplo, conecte-se \u00e0 inst\u00e2ncia (psql -h &lt;RDS_endpoint&gt; -U &lt;master_user&gt;) e rode o seguinte comando:<\/p>\n<p><code>CREATE DATABASE &lt;new_db_name&gt;;<\/code><\/p>\n<p>A partir de agora, voc\u00ea pode se conectar a esse banco de dados e us\u00e1-lo normalmente. Se voc\u00ea\u00a0configurou backups autom\u00e1ticos para essa inst\u00e2ncia, os backups cont\u00e9m todos os bancos de dados. Para listar os bancos de dados sob a inst\u00e2ncia, rode \u00a0<code>\\list<\/code> (ou\u00a0<code>\\l<\/code>). Para se conectar a outro banco, use o comando\u00a0<code>\\connect &lt;database_name&gt;<\/code>.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 29 Sep 2015 10:35:04 +0000","created_by":1,"updated_at":"Tue, 29 Sep 2015 10:35:04 +0000","updated_by":1,"published_at":"Tue, 29 Sep 2015 10:35:04 +0000","published_by":1},{"id":189,"title":"Monitorando um Processo com AWS CloudWatch","slug":"temp-slug-25","markdown":"\nO\u00a0CloudWatch da AWS \u00e9 uma \u00f3tima ferramenta para monitorar as suas inst\u00e2ncias EC2. Ele possui diversas m\u00e9tricas de desempenho e uso de recursos como disco, mem\u00f3ria, processamento etc. A ferramenta, por\u00e9m, vai al\u00e9m disso: \u00e9\u00a0poss\u00edvel usar a sua API para monitorar\u00a0diversos outros eventos, tais como processos e ********colocar outros eventos*********\n\nO CloudWatch \u00e9, segundo a pr\u00f3pria Amazon, um \u201creposit\u00f3rio de m\u00e9tricas\u201d. \u00a0Atrav\u00e9s\u00a0de chamadas da API, \u00e9 poss\u00edvel criar uma nova m\u00e9trica e aliment\u00e1-la com valores num\u00e9ricos. Uma vez criada, as m\u00e9tricas customizadas podem ser visualizadas da mesma foram que as m\u00e9tricas padr\u00e3o.\n\nQualquer valor num\u00e9rico que represente o estado do recurso que voc\u00ea deseja monitorar\u00a0pode ser usado aqui. Como exemplo, vamos\u00a0criar uma m\u00e9trica simples que monitora um processo a cada 5 minutos. Se o processo estiver sendo executado, o valor enviado \u00e9 1; caso contr\u00e1rio, o valor \u00e9 0.\n\n\n## Instala\u00e7\u00e3o e\u00a0Configura\u00e7\u00e3o\n\nAntes de criarmos o script para monitorar o processo, precisamos instalar o\n\n\u00a0\n\n\u00a0\n\n\n","html":"<p>O\u00a0CloudWatch da AWS \u00e9 uma \u00f3tima ferramenta para monitorar as suas inst\u00e2ncias EC2. Ele possui diversas m\u00e9tricas de desempenho e uso de recursos como disco, mem\u00f3ria, processamento etc. A ferramenta, por\u00e9m, vai al\u00e9m disso: \u00e9\u00a0poss\u00edvel usar a sua API para monitorar\u00a0diversos outros eventos, tais como processos e ********colocar outros eventos*********<\/p>\n<p>O CloudWatch \u00e9, segundo a pr\u00f3pria Amazon, um &#8220;reposit\u00f3rio de m\u00e9tricas&#8221;. \u00a0Atrav\u00e9s\u00a0de chamadas da API, \u00e9 poss\u00edvel criar uma nova m\u00e9trica e aliment\u00e1-la com valores num\u00e9ricos. Uma vez criada, as m\u00e9tricas customizadas podem ser visualizadas da mesma foram que as m\u00e9tricas padr\u00e3o.<\/p>\n<p>Qualquer valor num\u00e9rico que represente o estado do recurso que voc\u00ea deseja monitorar\u00a0pode ser usado aqui. Como exemplo, vamos\u00a0criar uma m\u00e9trica simples que monitora um processo a cada 5 minutos. Se o processo estiver sendo executado, o valor enviado \u00e9 1; caso contr\u00e1rio, o valor \u00e9 0.<\/p>\n<h2>Instala\u00e7\u00e3o e\u00a0Configura\u00e7\u00e3o<\/h2>\n<p>Antes de criarmos o script para monitorar o processo, precisamos instalar o<\/p>\n<p>&nbsp;<\/p>\n<p>&nbsp;<\/p>\n","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Fri, 27 Nov 2015 18:30:29 +0000","created_by":1,"updated_at":"Fri, 27 Nov 2015 18:30:29 +0000","updated_by":1,"published_at":"","published_by":1},{"id":199,"title":"High Price - Carl Hart","slug":"temp-slug-26","markdown":"\n- - - - - -\n\nCarl Hart is uniquely positioned to talk about drugs. His experience as an accomplished neuroscientist at *fix credentials* and his troubled childhood in Miami give him a perspective on the drugs matter that few will ever have. This position is highly explored in the book: the author intersperses his anecdotal experience with scientific data in order to debunk everything you think you know about drugs.\n\nOne of the biggest myths that the book tries to bust is the exaggerated importance given to how drugs affect brain\u2019s neuroreceivers. Trying to explain addiction purely on those terms means neglecting a big part of the data available on the subject. More than 50% of drug users do not end up as junkies who rob and kill for the next high, and the majority of people that try drugs \u2013 including the last three presidents of the United States \u2013 will probably never touch it again.\n\nThe roots of this misconception are the famous experiments with rats to determine the effects of cocaine in their brains. Rats were confined and given an infinite supply of sweetened water with cocaine. To get a shot, the rat must press a button. Soon enough, the rat is trading basically anything for the next high, which led scientists to conclude that cocaine is extremely addictive and would lead to similar behavior in humans.\n\nWhat these experiments fail to acknowledge is that the rats had no alternative. There they were, isolated from any contact with other animals (rats are very social), confined in an alien, almost hostile environment. When a similar study was conducted, replacing the cage for what he calls a \u201crat park\u201d \u2013 a space with activities, other rats etc \u2013 the addiction levels weren\u2019t even close to what was reported by the previous studies.\n\nThe isolation from the rest of the world and the lack of alternatives is what drives drug users in a downward spiral, and ironically enough, that\u2019s precisely what the drug war breeds. Minor drug offenders that were caught are far more likely to be involved in other crimes than the ones that got away \u2013 and there\u2019s a very strong racial component determining who will get caught and who won\u2019t. Not that crimes, big or small, should not be punished \u2013 they should, but in a way that allows redemption.\n\n\n","html":"<hr \/>\n<p style=\"text-align: left;\">Carl Hart is uniquely positioned to talk about drugs. His experience as an accomplished neuroscientist at *fix credentials* and his troubled childhood in Miami give him a perspective on the drugs matter that few will ever have. This position is highly explored in the book: the author intersperses his anecdotal experience with scientific data in order to debunk everything you think you know about drugs.<\/p>\n<p style=\"text-align: left;\">One of the biggest myths that the book tries to bust is the exaggerated importance given to how drugs affect brain&#8217;s neuroreceivers. Trying to explain addiction purely on those terms means neglecting a big part of the data available on the subject. More than 50% of drug users do not end up as junkies who rob and kill for the next high, and the majority of people that try drugs &#8211; including the last three presidents of the United States &#8211; will probably never touch it again.<\/p>\n<p style=\"text-align: left;\">The roots of this misconception are the famous experiments with rats to determine the effects of cocaine in their brains. Rats were confined and given an infinite supply of sweetened water with cocaine. To get a shot, the rat must press a button. Soon enough, the rat is trading basically anything for the next high, which led scientists to conclude that cocaine is extremely addictive and would lead to similar behavior in humans.<\/p>\n<p style=\"text-align: left;\">What these experiments fail to acknowledge is that the rats had no alternative. There they were, isolated from any contact with other animals (rats are very social), confined in an alien, almost hostile environment. When a similar study was conducted, replacing the cage for what he calls a &#8220;rat park&#8221; &#8211; a space with activities, other rats etc &#8211; the addiction levels weren&#8217;t even close to what was reported by the previous studies.<\/p>\n<p style=\"text-align: left;\">The isolation from the rest of the world and the lack of alternatives is what drives drug users in a downward spiral, and ironically enough, that&#8217;s precisely what the drug war breeds. Minor drug offenders that were caught are far more likely to be involved in other crimes than the ones that got away &#8211; and there&#8217;s a very strong racial component determining who will get caught and who won&#8217;t. Not that crimes, big or small, should not be punished &#8211; they should, but in a way that allows redemption.<\/p>\n","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Sat, 02 Jan 2016 09:33:49 +0000","created_by":1,"updated_at":"Sat, 02 Jan 2016 09:33:49 +0000","updated_by":1,"published_at":"","published_by":1},{"id":201,"title":"Witchcraft - Nucleus","slug":"temp-slug-27","markdown":"\nWitchcraft is back after a 4-year\u00a0hiatus with their new album Nucleus. While their previous efforts were more geared towards classic rock, this album is pretty heavy, with slow tempos that give it a very stoner feeling. Although it\u00a0sounds very raw and not overproduced, there are some incursions from other\u00a0instruments such as flute, cello and keyboard. These incursions, along with\u00a0long, complex songs with\u00a0lots of variations in tempo and mood, show an attempt to evolve from the straightforward \u201cseventies\u2019 rock and roll\u201d mold.\u00a0This evolution was already showing up in Legend, but it\u2019s way more pronounced in Nucleus.\n\nMaelstroem, opens the album\u00a0with a beautiful acoustic guitar and flute intro which slowly turns\u00a0into a stoner tune, with heavy riffs and slow tempo. An 8-minute track is\u00a0not what you would expect for an\u00a0album opener, but Witchcraft doesn\u2019t appear to give a fuck\u00a0for what you expect. Good, melodic vocals with a sort of guitar overdrive from Magnus Pelander.\n\nTheory of Consequence\n\nThe Outcast, the previously released single, is\u00a0full of groove and a beautiful chorus, again with some participation of flutes. It\u2019s probably the \u201cleast heavy\u201d song of the album. By the\u00a0middle of the track it takes a turn\u00a0into an almost completely different song, with a more classic rock and roll beat. Nice guitar solos all over the track.\u00a0One of my favorite tracks.\n\nNucleus. Long acoustic guitar intro, with some string arrangements\n\nAn Exorcism of Doubts: Black Sabbath feeling, heavy and slow, bluesy mood\n\nThe Obsessed: Punchy, classic rock and roll to the core. Few overdubs, distorted bass guitar doing all the base work.\n\nTo Transcend Bitterness\n\nHelpless: Doom, heavy\n\nBreakdown:\n\n\n","html":"<p>Witchcraft is back after a 4-year\u00a0hiatus with their new album Nucleus. While their previous efforts were more geared towards classic rock, this album is pretty heavy, with slow tempos that give it a very stoner feeling. Although it\u00a0sounds very raw and not overproduced, there are some incursions from other\u00a0instruments such as flute, cello and keyboard. These incursions, along with\u00a0long, complex songs with\u00a0lots of variations in tempo and mood, show an attempt to evolve from the straightforward &#8220;seventies&#8217; rock and roll&#8221; mold.\u00a0This evolution was already showing up in Legend, but it&#8217;s way more pronounced in Nucleus.<\/p>\n<p>Maelstroem, opens the album\u00a0with a beautiful acoustic guitar and flute intro which slowly turns\u00a0into a stoner tune, with heavy riffs and slow tempo. An 8-minute track is\u00a0not what you would expect for an\u00a0album opener, but Witchcraft doesn&#8217;t appear to give a fuck\u00a0for what you expect. Good, melodic vocals with a sort of guitar overdrive from Magnus Pelander.<\/p>\n<p>Theory of Consequence<\/p>\n<p>The Outcast, the previously released single, is\u00a0full of groove and a beautiful chorus, again with some participation of flutes. It&#8217;s probably the &#8220;least heavy&#8221; song of the album. By the\u00a0middle of the track it takes a turn\u00a0into an almost completely different song, with a more classic rock and roll beat. Nice guitar solos all over the track.\u00a0One of my favorite tracks.<\/p>\n<p>Nucleus. Long acoustic guitar intro, with some string arrangements<\/p>\n<p>An Exorcism of Doubts: Black Sabbath feeling, heavy and slow, bluesy mood<\/p>\n<p>The Obsessed: Punchy, classic rock and roll to the core. Few overdubs, distorted bass guitar doing all the base work.<\/p>\n<p>To Transcend Bitterness<\/p>\n<p>Helpless: Doom, heavy<\/p>\n<p>Breakdown:<\/p>\n","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 19 Jan 2016 16:46:00 +0000","created_by":1,"updated_at":"Tue, 19 Jan 2016 16:46:00 +0000","updated_by":1,"published_at":"","published_by":1},{"id":204,"title":"Tutorial - Adding HTTPS to a Wordpress website","slug":"tutorial-adding-https-to-a-wordpress-website","markdown":"\nSince Google [announced that it would\u00a0boost the rank of websites with https](https:\/\/googlewebmastercentral.blogspot.com.br\/2014\/08\/https-as-ranking-signal.html), even websites that do not deal with sensitive data have a motivation to install a SSL certificate. In this tutorial I\u2019ll go through the whole process of creation and installation of an SSL certificate to a self-hosted WordPress website running on CentOS.\n\n\n## Purchasing the SSL Certificate\n\nThe first step\u00a0is the purchase. You have to decide\u00a0whether you want a wildcard certificate or not. The difference is that the wildcard certificate will certify all subdomains you may have, whilst the standard certificate will only cover the domain you provide.\u00a0A wild card certificate would allow\u00a0me to have https:\/\/db.rafaelmt.github.io for example, while the standard one would not.\n\nThere are plenty of SSL certificate issuers\u00a0around, many of them with very low cost for 1 year of the standard\u00a0certificates. I went with GoDaddy, but the process should be\u00a0similar for other providers. The purchase process is very\u00a0straightforward. Don\u2019t worry if you don\u2019t get asked\u00a0which domain you want to get certified at this point, you will provide this info later.\n\n\n## Generating the Request\n\nOnce you finished the checkout, you\u2019ll\u00a0be asked to provide a .csr file. This file is generated along with the private key\u00a0of your certificate and it contains information about your domain, such as\u00a0its name, location of your server etc.\n\nTo generate the .csr file, execute the following command in your server:\n\nopenssl req -new -newkey rsa:2048 -nodes -keyout server.key -out server.csr\n\nIt will ask you to fill up the information about the certificate. When done, it will generate two files: the server.key, which is the private key of your certificate, and the server.csr, which is the file containing the request. The private key should not be shared with anyone! If you lose this file, you will have to re-generate the certificate.\n\n\n## Verify Domain Ownership\n\nYour certificate issuer will ask you to verify the ownership of your domain. The method may vary, but the most common one is requesting you to\u00a0create a file with a given file name in the root of your domain.\n\nSome certificate issuers will also request you to provide some documentation to verify your identity.\n\n\n## Download and Install Certificate\n\nOnce the domain ownership is verified, the issuer will allow you to download your certificate. You will probably get two .crt files, one containing the certificate (let\u2019s call it server.crt), and another one containing the\u00a0certificate chain. The certificate file is the one that has only one certificate block (`-----BEGIN CERTIFICATE----- ... -----END CERTIFICATE-----`).\n\nCopy the server.crt file to your server and place both server.crt and server.key in a place where Apache server can access. The typical location for the files are  \n```\n\/etc\/ssl\/certs\/server.crt<br><\/br>\n\/etc\/ssl\/private\/server.key<br><\/br>```\n\n\n## Configure HTTPD\n\nAdd the following virtual host configuration to your \/etc\/httpd\/conf\/httpd.conf, changing the ServerName line to add your domain name:  \n [gist\u00a01dd778987d936944bacf]  \n Make sure the following lines are present in your configuration:\n\nLoadModule ssl_module modules\/mod_ssl.so\n\nListen 443 https\n\nYou\u00a0also need to install the mod_ssl:\n\nsudo yum install mod_ssl\n\nValidate your configuration file by running the following command:\n\nsudo service httpd configtest\n\n\n## Restart Apache\u00a0and Test!\n\nNow let\u2019s restart the\u00a0apache server and\u00a0test the https:\n\nsudo service httpd restart\n\nFire up a browser and try to reach your server. Make sure you have opened the port 443 in\u00a0iptables.\n\n\n## Redirect HTTP to HTTPS\n\nOnce you\u2019re sure everything is working fine, you have to make sure\u00a0users that access your website through http get redirected to https. To do so, add the following configuration to\u00a0\/etc\/httpd\/conf\/httpd.conf:  \n [gist\u00a067c8e639bef4cebd2e4d]\n\n\n","html":"<p>Since Google <a href=\"https:\/\/googlewebmastercentral.blogspot.com.br\/2014\/08\/https-as-ranking-signal.html\">announced that it would\u00a0boost the rank of websites with https<\/a>, even websites that do not deal with sensitive data have a motivation to install a SSL certificate. In this tutorial I&#8217;ll go through the whole process of creation and installation of an SSL certificate to a self-hosted WordPress website running on CentOS.<\/p>\n<h2>Purchasing the SSL Certificate<\/h2>\n<p>The first step\u00a0is the purchase. You have to decide\u00a0whether you want a wildcard certificate or not. The difference is that the wildcard certificate will certify all subdomains you may have, whilst the standard certificate will only cover the domain you provide.\u00a0A wild card certificate would allow\u00a0me to have https:\/\/db.rafaelmt.github.io for example, while the standard one would not.<\/p>\n<p>There are plenty of SSL certificate issuers\u00a0around, many of them with very low cost for 1 year of the standard\u00a0certificates. I went with GoDaddy, but the process should be\u00a0similar for other providers. The purchase process is very\u00a0straightforward. Don&#8217;t worry if you don&#8217;t get asked\u00a0which domain you want to get certified at this point, you will provide this info later.<\/p>\n<h2>Generating the Request<\/h2>\n<p>Once you finished the checkout, you&#8217;ll\u00a0be asked to provide a .csr file. This file is generated along with the private key\u00a0of your certificate and it contains information about your domain, such as\u00a0its name, location of your server etc.<\/p>\n<p>To generate the .csr file, execute the following command in your server:<\/p>\n<pre class=\"wp-code-highlight prettyprint\">openssl req -new -newkey rsa:2048 -nodes -keyout server.key -out server.csr<\/pre>\n<p>It will ask you to fill up the information about the certificate. When done, it will generate two files: the server.key, which is the private key of your certificate, and the server.csr, which is the file containing the request. The private key should not be shared with anyone! If you lose this file, you will have to re-generate the certificate.<\/p>\n<h2>Verify Domain Ownership<\/h2>\n<p>Your certificate issuer will ask you to verify the ownership of your domain. The method may vary, but the most common one is requesting you to\u00a0create a file with a given file name in the root of your domain.<\/p>\n<p>Some certificate issuers will also request you to provide some documentation to verify your identity.<\/p>\n<h2>Download and Install Certificate<\/h2>\n<p>Once the domain ownership is verified, the issuer will allow you to download your certificate. You will probably get two .crt files, one containing the certificate (let&#8217;s call it server.crt), and another one containing the\u00a0certificate chain. The certificate file is the one that has only one certificate block (<code>-----BEGIN CERTIFICATE----- ... -----END CERTIFICATE-----<\/code>).<\/p>\n<p>Copy the server.crt file to your server and place both server.crt and server.key in a place where Apache server can access. The typical location for the files are<br \/>\n<code>\/etc\/ssl\/certs\/server.crt<br \/>\n\/etc\/ssl\/private\/server.key<br \/>\n<\/code><\/p>\n<h2>Configure HTTPD<\/h2>\n<p>Add the following virtual host configuration to your \/etc\/httpd\/conf\/httpd.conf, changing the ServerName line to add your domain name:<br \/>\n[gist\u00a01dd778987d936944bacf]<br \/>\nMake sure the following lines are present in your configuration:<\/p>\n<pre class=\"wp-code-highlight prettyprint\">LoadModule ssl_module modules\/mod_ssl.so<\/pre>\n<pre class=\"wp-code-highlight prettyprint\">Listen 443 https<\/pre>\n<p>You\u00a0also need to install the mod_ssl:<\/p>\n<pre class=\"wp-code-highlight prettyprint\">sudo yum install mod_ssl<\/pre>\n<p>Validate your configuration file by running the following command:<\/p>\n<pre class=\"wp-code-highlight prettyprint\">sudo service httpd configtest<\/pre>\n<h2>Restart Apache\u00a0and Test!<\/h2>\n<p>Now let&#8217;s restart the\u00a0apache server and\u00a0test the https:<\/p>\n<pre class=\"wp-code-highlight prettyprint\">sudo service httpd restart<\/pre>\n<p>Fire up a browser and try to reach your server. Make sure you have opened the port 443 in\u00a0iptables.<\/p>\n<h2>Redirect HTTP to HTTPS<\/h2>\n<p>Once you&#8217;re sure everything is working fine, you have to make sure\u00a0users that access your website through http get redirected to https. To do so, add the following configuration to\u00a0\/etc\/httpd\/conf\/httpd.conf:<br \/>\n[gist\u00a067c8e639bef4cebd2e4d]<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Wed, 20 Jan 2016 13:38:11 +0000","created_by":1,"updated_at":"Tue, 02 Feb 2016 14:34:41 +0000","updated_by":1,"published_at":"Wed, 20 Jan 2016 13:38:11 +0000","published_by":1},{"id":222,"title":"Staying Alive with Monit - Installation and Configuration","slug":"temp-slug-29","markdown":"\n[Monit](https:\/\/mmonit.com\/monit\/) is a very simple yet powerful tool to monitor processes in Linux systems. I\u00a0use it to keep this blog up and it works great. It\u2019s open source and bundled in all major distros.\n\n\u00a0\n\n\n## Installation\n\nIt\u2019 couldn\u2019t be easier:\n\n`sudo yum install monit`\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n\n","html":"<p><a href=\"https:\/\/mmonit.com\/monit\/\">Monit<\/a> is a very simple yet powerful tool to monitor processes in Linux systems. I\u00a0use it to keep this blog up and it works great. It&#8217;s open source and bundled in all major distros.<\/p>\n<p>&nbsp;<\/p>\n<h2>Installation<\/h2>\n<p>It&#8217; couldn&#8217;t be easier:<\/p>\n<p><code>sudo yum install monit<\/code><\/p>\n<p>&nbsp;<\/p>\n<p>&nbsp;<\/p>\n<p>&nbsp;<\/p>\n","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Thu, 21 Jan 2016 11:41:23 +0000","created_by":1,"updated_at":"Thu, 21 Jan 2016 11:41:23 +0000","updated_by":1,"published_at":"","published_by":1},{"id":225,"title":"Don't overengineer and get stuff done","slug":"dont-overengineer-and-get-stuff-done","markdown":"\nWhile\u00a0reading Umberto Eco\u2019s Numero Zero\u00a0I\u00a0came across\u00a0a funny conversation that many software engineers will probably relate. Colonna, the\u00a0main character and narrator, is talking to Romano Braggadocio,\u00a0a journalist he just met. Braggadocio\u00a0starts telling him about some story he is writing about that\u2019s going to be so huge it might even turn into a book. There\u2019s one thing, however, keeping him from finishing the required investigations: he\u00a0needs a car.\n\nBraggadocio\u00a0then proceeds to describe all the features he looks for in a car. It has to be fast, sturdy, not too wide so to not fit an average garage, but not too narrow so to not be tight for a big man like himself. It has to be low-maintenance\u00a0and\u00a0quickly\u00a0available but not too quickly\u00a0(this would mean nobody wants the car, he claims). \u00a0He enumerates\u00a0a lot of candidate cars, but none fits the perfect model of car he envisions.\n\nAs a result, his investigations never happen and the \u201cbig story\u201d never comes through. The funny thing is, none of the\u00a0features of the car would directly affect his investigations or keep him from doing them.\u00a0You don\u2019t need a car that goes from 0 to 100km\/h in 7.5 seconds to run a few\u00a0errands; one that does it in 8.5 seconds would do just fine.\n\nI saw\u00a0an immediate parallel between this conversation\u00a0and a number of\u00a0personal\/side projects that me and lots of colleagues have started but never finished. Everyone\u00a0has cool ideas of new apps, web sites, services\u00a0etc, but when it comes to implement it, some people get lost in an internal debate on whether they should use database A or B, which framework would scale better for 10 million users, what\u2019s the best language for concurrency etc. While you do it, your idea is still just an idea.\n\nEngineers tend to overthink when they are starting\u00a0new projects. As inherently lazy people, we\u00a0dread the idea of\u00a0having to rewrite stuff because the\u00a0original implementation\u00a0doesn\u2019t scale well, or having to revamp the\u00a0whole architecture because it does not support\u00a0the load anymore. What we sometimes fail to grasp is that, in order to have those problems, the\u00a0product has to exist. If you ever hit one of those performance walls, you\u2019ll already have a somewhat successful product, which is certainly better than having just an idea.\n\nDon\u2019t get me wrong, you can\u2019t just randomly make decisions at the beginning of your project and hope for the best. Some rationale must be used to back your architectural\/design choices. Just don\u2019t let these decisions\u00a0take too much of your time.\u00a0Sam Altman from Y Combinator in his \u201c[Startup Playbook](http:\/\/playbook.samaltman.com\/)\u201d (http:\/\/playbook.samaltman.com\/) recommends\u00a0building things that would work on a\u00a0scale 10x bigger than your current one. I think this is a great rule of thumb.\n\n\u00a0\n\n\u00a0\n\n\n","html":"<p>While\u00a0reading Umberto Eco&#8217;s Numero Zero\u00a0I\u00a0came across\u00a0a funny conversation that many software engineers will probably relate. Colonna, the\u00a0main character and narrator, is talking to Romano Braggadocio,\u00a0a journalist he just met. Braggadocio\u00a0starts telling him about some story he is writing about that&#8217;s going to be so huge it might even turn into a book. There&#8217;s one thing, however, keeping him from finishing the required investigations: he\u00a0needs a car.<\/p>\n<p>Braggadocio\u00a0then proceeds to describe all the features he looks for in a car. It has to be fast, sturdy, not too wide so to not fit an average garage, but not too narrow so to not be tight for a big man like himself. It has to be low-maintenance\u00a0and\u00a0quickly\u00a0available but not too quickly\u00a0(this would mean nobody wants the car, he claims). \u00a0He enumerates\u00a0a lot of candidate cars, but none fits the perfect model of car he envisions.<\/p>\n<p>As a result, his investigations never happen and the &#8220;big story&#8221; never comes through. The funny thing is, none of the\u00a0features of the car would directly affect his investigations or keep him from doing them.\u00a0You don&#8217;t need a car that goes from 0 to 100km\/h in 7.5 seconds to run a few\u00a0errands; one that does it in 8.5 seconds would do just fine.<\/p>\n<p>I saw\u00a0an immediate parallel between this conversation\u00a0and a number of\u00a0personal\/side projects that me and lots of colleagues have started but never finished. Everyone\u00a0has cool ideas of new apps, web sites, services\u00a0etc, but when it comes to implement it, some people get lost in an internal debate on whether they should use database A or B, which framework would scale better for 10 million users, what&#8217;s the best language for concurrency etc. While you do it, your idea is still just an idea.<\/p>\n<p>Engineers tend to overthink when they are starting\u00a0new projects. As inherently lazy people, we\u00a0dread the idea of\u00a0having to rewrite stuff because the\u00a0original implementation\u00a0doesn&#8217;t scale well, or having to revamp the\u00a0whole architecture because it does not support\u00a0the load anymore. What we sometimes fail to grasp is that, in order to have those problems, the\u00a0product has to exist. If you ever hit one of those performance walls, you&#8217;ll already have a somewhat successful product, which is certainly better than having just an idea.<\/p>\n<p>Don&#8217;t get me wrong, you can&#8217;t just randomly make decisions at the beginning of your project and hope for the best. Some rationale must be used to back your architectural\/design choices. Just don&#8217;t let these decisions\u00a0take too much of your time.\u00a0Sam Altman from Y Combinator in his &#8220;<a href=\"http:\/\/playbook.samaltman.com\/\">Startup Playbook<\/a>&#8221; (http:\/\/playbook.samaltman.com\/) recommends\u00a0building things that would work on a\u00a0scale 10x bigger than your current one. I think this is a great rule of thumb.<\/p>\n<p>&nbsp;<\/p>\n<p>&nbsp;<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 02 Feb 2016 14:33:49 +0000","created_by":1,"updated_at":"Tue, 02 Feb 2016 14:33:49 +0000","updated_by":1,"published_at":"Tue, 02 Feb 2016 14:33:49 +0000","published_by":1},{"id":231,"title":"Scala Relate","slug":"temp-slug-31","markdown":"\nhttps:\/\/github.com\/lucidsoftware\/relate\/wiki\n\nhttps:\/\/www.lucidchart.com\/techblog\/2014\/06\/17\/performant-database-access-relate\/\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n\n","html":"<p>https:\/\/github.com\/lucidsoftware\/relate\/wiki<\/p>\n<p>https:\/\/www.lucidchart.com\/techblog\/2014\/06\/17\/performant-database-access-relate\/<\/p>\n<p>&nbsp;<\/p>\n<p>&nbsp;<\/p>\n<p>&nbsp;<\/p>\n","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Thu, 11 Feb 2016 09:48:16 +0000","created_by":1,"updated_at":"Thu, 11 Feb 2016 09:48:16 +0000","updated_by":1,"published_at":"","published_by":1},{"id":236,"title":"Home","slug":"temp-slug-32","markdown":"","html":"","image":null,"featured":0,"page":1,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Mon, 03 Oct 2016 12:33:23 +0000","created_by":1,"updated_at":"Mon, 03 Oct 2016 12:33:23 +0000","updated_by":1,"published_at":"","published_by":1}],"tags":[{"id":51,"name":"aws","slug":"aws","description":""},{"id":37,"name":"elasticsearch","slug":"elasticsearch","description":""},{"id":59,"name":"elasticsearch","slug":"elasticsearch-en","description":""},{"id":21,"name":"elk","slug":"elk","description":""},{"id":61,"name":"elk","slug":"elk-en","description":""},{"id":35,"name":"kibana","slug":"kibana","description":""},{"id":63,"name":"kibana","slug":"kibana-en","description":""},{"id":53,"name":"linux","slug":"linux","description":""},{"id":23,"name":"logstash","slug":"logstash","description":""},{"id":65,"name":"logstash","slug":"logstash-en","description":""},{"id":55,"name":"script","slug":"script","description":""},{"id":33,"name":"tutorial","slug":"tutorial","description":""},{"id":67,"name":"tutorial","slug":"tutorial-en","description":""}],"posts_tags":[{"tag_id":37,"post_id":12},{"tag_id":21,"post_id":12},{"tag_id":35,"post_id":12},{"tag_id":23,"post_id":12},{"tag_id":33,"post_id":12},{"tag_id":59,"post_id":85},{"tag_id":61,"post_id":85},{"tag_id":63,"post_id":85},{"tag_id":65,"post_id":85},{"tag_id":67,"post_id":85},{"tag_id":37,"post_id":92},{"tag_id":21,"post_id":92},{"tag_id":35,"post_id":92},{"tag_id":23,"post_id":92},{"tag_id":33,"post_id":92},{"tag_id":51,"post_id":140},{"tag_id":53,"post_id":140},{"tag_id":55,"post_id":140},{"tag_id":37,"post_id":145},{"tag_id":21,"post_id":145},{"tag_id":35,"post_id":145},{"tag_id":23,"post_id":145},{"tag_id":49,"post_id":145},{"tag_id":37,"post_id":5},{"tag_id":21,"post_id":5},{"tag_id":35,"post_id":5},{"tag_id":23,"post_id":5},{"tag_id":33,"post_id":5}],"users":[{"id":1,"slug":"rafaelmt","bio":false,"website":"http:\/\/rafaelmt.github.io","created_at":"Thu, 13 Aug 2015 16:36:11 +0000","created_by":1,"email":"rafaelmedtex@gmail.com","name":"rafaelmt"}]},"meta":{"exported_on":"Sat, 15 Oct 2016 11:55:10 +0000","version":"000"}}