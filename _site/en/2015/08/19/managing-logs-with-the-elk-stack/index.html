<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.4.0 -->
<title>Tutorial: Managing Logs with the ELK Stack | Rafael M. Teixeira</title>
<meta name="generator" content="Jekyll v3.7.3" />
<meta property="og:title" content="Tutorial: Managing Logs with the ELK Stack" />
<meta name="author" content="Rafael M Teixeira" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Consolidating, indexing and analyzing your environment’s logs is fundamental. Gone are the days when truckloads of logs were stored only to be used when something went wrong. It’s possible to extract a lot of information from your infrastructure and applications’ logs, allowing you to rapidly detect abnormalities, foresee problems and even support business decisions. There are plenty of cloud log management services around: Loggly, Papertrail e Logentries are some of the most used. Their goal is to be as simple as it gets: create an account, redirect your logs to the provided endpoint and that’s it: the rest is on their side. I’ve used these three and I might write about them some day, but we have another objective in this post: I’ll show how you can create a flexible and powerful log manager using the Elasticsearch/Logstash/Kibana stack. ELK? All three components of the ELK stack are open source (Apache 2 license) and kept by Elastic, allowing modification, distribution and commercial use. The components are: Elasticsearch: distributed search engine based on Apache Lucene. It has a RESTful API as interface, with JSON objects. Logstash: data pipeline tool. Centralizes, normalizes and processes data from different sources. Kibana: data visualization web interface. Allows creating graphs and filters for data indexed by Elasticsearch. This is the data flow throughout the stack: data is generated and sent to Logstash. Logstash reads, parses and filters the data, then sends it to Elasticsearch. Kibana uses Elasticsearch’s  indexes to search data and displays it in a user-friendly way. In this example, we’ll use an Apache log as input. The flow is as follows:   Installation I’ve used Centos 7 for this tutorial. The commands should work in any RHEL-based distro. Java Logstash runs both on Oracle JDK and OpenJDK. We’ll use OpenJDK: sudo yum install java-1.8.0-openjdk.x86_64 The stack installation process is pretty straightforward: simply download the components and decompress the files. I’ll put them all in the /opt/ folder. I’m using the most recent versions (at the time of writing). Elasticsearch curl -O https://download.elastic.co/elasticsearch/elasticsearch/elasticsearch-1.7.1.tar.gz sudo tar -xzvf elasticsearch-1.7.1.tar.gz -C /opt/ Logstash curl -O https://download.elastic.co/logstash/logstash/logstash-1.5.3.tar.gz sudo tar -xzvf logstash-1.5.3.tar.gz -C /opt/ Kibana curl -O https://download.elastic.co/kibana/kibana/kibana-4.1.1-linux-x64.tar.gz sudo tar -xzvf kibana-4.1.1-linux-x64.tar.gz -C /opt/ Creating the Logstash pipeline To create a Logstash data pipeline, we need to specify the inputs, filters and outputs.  The inputs and outputs read and write data, while the filters adapt the data format to what the output expects. Logstash’s architecture allows installing inputs, filters and outputs as plugins, as well as creating your own. Let’s create a pipeline with a an Apache access log as input and Elasticsearch as output. The scenario is as follows: input: reads a file containing Apache access log filter: parses each log line in a JSON object to be used as input in Elasticsearch output: sends the JSON objects to an Elasticsearch instance We’ll use the Grok filter to transform text from the log file into structured data. Grok allows parsing input text using patterns. It ships with a number of different patterns, Apache being one of them. The file input transforms each line of the log file in an event. The Grok filter reads the events, finds the fields matching the pattern (ip, request type, response etc.) and adds them to the event. The Elasticsearch output gets the events, transforms them into JSON objects and sends it to an Elasticsearch instance. This is the configuration file for the pipeline: [gist 4e0c4561471f8a00f428] Change /path/to/apache_log to (guess what …) the path where apache log is in your machine and save it. If you don’t have an Apache log to test, use this one. Decompress it with tar -xzvf access_log.tgz You might have noticed that, contrary to what one could expect, the elasticsearch output does not specify the Elasticsearch instance’s address. Since Elasticsearch is running in the same machine as Logstash and is configured with Multicast enabled by default, it will be automatically found. This configuration is not recommended for production environments. Running Let’s start by initializing Elasticsearch: &lt;elasticsearch_root_folder&gt;/bin/elasticsearch -d Now Logstash: &lt;logstash_root_folder&gt;/bin/logstash -f pipeline.conf &amp; And now Kibana. Notice that we didn’t edit any configuration files for Kibana: by default, it will look for an Elasticsearch instance in localhost:9200 (which is exactly where our elasticsearch is). &lt;kibana_root_folder&gt;/bin/kibana &amp; Now browse to the port 5601 of your machine. You should see something like this: There you go! You have an open source, flexible and scalable log management system. You can read a little more about Kibana usage in this post.   [yasr_visitor_votes size=”small”]" />
<meta property="og:description" content="Consolidating, indexing and analyzing your environment’s logs is fundamental. Gone are the days when truckloads of logs were stored only to be used when something went wrong. It’s possible to extract a lot of information from your infrastructure and applications’ logs, allowing you to rapidly detect abnormalities, foresee problems and even support business decisions. There are plenty of cloud log management services around: Loggly, Papertrail e Logentries are some of the most used. Their goal is to be as simple as it gets: create an account, redirect your logs to the provided endpoint and that’s it: the rest is on their side. I’ve used these three and I might write about them some day, but we have another objective in this post: I’ll show how you can create a flexible and powerful log manager using the Elasticsearch/Logstash/Kibana stack. ELK? All three components of the ELK stack are open source (Apache 2 license) and kept by Elastic, allowing modification, distribution and commercial use. The components are: Elasticsearch: distributed search engine based on Apache Lucene. It has a RESTful API as interface, with JSON objects. Logstash: data pipeline tool. Centralizes, normalizes and processes data from different sources. Kibana: data visualization web interface. Allows creating graphs and filters for data indexed by Elasticsearch. This is the data flow throughout the stack: data is generated and sent to Logstash. Logstash reads, parses and filters the data, then sends it to Elasticsearch. Kibana uses Elasticsearch’s  indexes to search data and displays it in a user-friendly way. In this example, we’ll use an Apache log as input. The flow is as follows:   Installation I’ve used Centos 7 for this tutorial. The commands should work in any RHEL-based distro. Java Logstash runs both on Oracle JDK and OpenJDK. We’ll use OpenJDK: sudo yum install java-1.8.0-openjdk.x86_64 The stack installation process is pretty straightforward: simply download the components and decompress the files. I’ll put them all in the /opt/ folder. I’m using the most recent versions (at the time of writing). Elasticsearch curl -O https://download.elastic.co/elasticsearch/elasticsearch/elasticsearch-1.7.1.tar.gz sudo tar -xzvf elasticsearch-1.7.1.tar.gz -C /opt/ Logstash curl -O https://download.elastic.co/logstash/logstash/logstash-1.5.3.tar.gz sudo tar -xzvf logstash-1.5.3.tar.gz -C /opt/ Kibana curl -O https://download.elastic.co/kibana/kibana/kibana-4.1.1-linux-x64.tar.gz sudo tar -xzvf kibana-4.1.1-linux-x64.tar.gz -C /opt/ Creating the Logstash pipeline To create a Logstash data pipeline, we need to specify the inputs, filters and outputs.  The inputs and outputs read and write data, while the filters adapt the data format to what the output expects. Logstash’s architecture allows installing inputs, filters and outputs as plugins, as well as creating your own. Let’s create a pipeline with a an Apache access log as input and Elasticsearch as output. The scenario is as follows: input: reads a file containing Apache access log filter: parses each log line in a JSON object to be used as input in Elasticsearch output: sends the JSON objects to an Elasticsearch instance We’ll use the Grok filter to transform text from the log file into structured data. Grok allows parsing input text using patterns. It ships with a number of different patterns, Apache being one of them. The file input transforms each line of the log file in an event. The Grok filter reads the events, finds the fields matching the pattern (ip, request type, response etc.) and adds them to the event. The Elasticsearch output gets the events, transforms them into JSON objects and sends it to an Elasticsearch instance. This is the configuration file for the pipeline: [gist 4e0c4561471f8a00f428] Change /path/to/apache_log to (guess what …) the path where apache log is in your machine and save it. If you don’t have an Apache log to test, use this one. Decompress it with tar -xzvf access_log.tgz You might have noticed that, contrary to what one could expect, the elasticsearch output does not specify the Elasticsearch instance’s address. Since Elasticsearch is running in the same machine as Logstash and is configured with Multicast enabled by default, it will be automatically found. This configuration is not recommended for production environments. Running Let’s start by initializing Elasticsearch: &lt;elasticsearch_root_folder&gt;/bin/elasticsearch -d Now Logstash: &lt;logstash_root_folder&gt;/bin/logstash -f pipeline.conf &amp; And now Kibana. Notice that we didn’t edit any configuration files for Kibana: by default, it will look for an Elasticsearch instance in localhost:9200 (which is exactly where our elasticsearch is). &lt;kibana_root_folder&gt;/bin/kibana &amp; Now browse to the port 5601 of your machine. You should see something like this: There you go! You have an open source, flexible and scalable log management system. You can read a little more about Kibana usage in this post.   [yasr_visitor_votes size=”small”]" />
<link rel="canonical" href="http://localhost:4000/en/2015/08/19/managing-logs-with-the-elk-stack/" />
<meta property="og:url" content="http://localhost:4000/en/2015/08/19/managing-logs-with-the-elk-stack/" />
<meta property="og:site_name" content="Rafael M. Teixeira" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2015-08-20T01:53:32+02:00" />
<script type="application/ld+json">
{"description":"Consolidating, indexing and analyzing your environment’s logs is fundamental. Gone are the days when truckloads of logs were stored only to be used when something went wrong. It’s possible to extract a lot of information from your infrastructure and applications’ logs, allowing you to rapidly detect abnormalities, foresee problems and even support business decisions. There are plenty of cloud log management services around: Loggly, Papertrail e Logentries are some of the most used. Their goal is to be as simple as it gets: create an account, redirect your logs to the provided endpoint and that’s it: the rest is on their side. I’ve used these three and I might write about them some day, but we have another objective in this post: I’ll show how you can create a flexible and powerful log manager using the Elasticsearch/Logstash/Kibana stack. ELK? All three components of the ELK stack are open source (Apache 2 license) and kept by Elastic, allowing modification, distribution and commercial use. The components are: Elasticsearch: distributed search engine based on Apache Lucene. It has a RESTful API as interface, with JSON objects. Logstash: data pipeline tool. Centralizes, normalizes and processes data from different sources. Kibana: data visualization web interface. Allows creating graphs and filters for data indexed by Elasticsearch. This is the data flow throughout the stack: data is generated and sent to Logstash. Logstash reads, parses and filters the data, then sends it to Elasticsearch. Kibana uses Elasticsearch’s  indexes to search data and displays it in a user-friendly way. In this example, we’ll use an Apache log as input. The flow is as follows:   Installation I’ve used Centos 7 for this tutorial. The commands should work in any RHEL-based distro. Java Logstash runs both on Oracle JDK and OpenJDK. We’ll use OpenJDK: sudo yum install java-1.8.0-openjdk.x86_64 The stack installation process is pretty straightforward: simply download the components and decompress the files. I’ll put them all in the /opt/ folder. I’m using the most recent versions (at the time of writing). Elasticsearch curl -O https://download.elastic.co/elasticsearch/elasticsearch/elasticsearch-1.7.1.tar.gz sudo tar -xzvf elasticsearch-1.7.1.tar.gz -C /opt/ Logstash curl -O https://download.elastic.co/logstash/logstash/logstash-1.5.3.tar.gz sudo tar -xzvf logstash-1.5.3.tar.gz -C /opt/ Kibana curl -O https://download.elastic.co/kibana/kibana/kibana-4.1.1-linux-x64.tar.gz sudo tar -xzvf kibana-4.1.1-linux-x64.tar.gz -C /opt/ Creating the Logstash pipeline To create a Logstash data pipeline, we need to specify the inputs, filters and outputs.  The inputs and outputs read and write data, while the filters adapt the data format to what the output expects. Logstash’s architecture allows installing inputs, filters and outputs as plugins, as well as creating your own. Let’s create a pipeline with a an Apache access log as input and Elasticsearch as output. The scenario is as follows: input: reads a file containing Apache access log filter: parses each log line in a JSON object to be used as input in Elasticsearch output: sends the JSON objects to an Elasticsearch instance We’ll use the Grok filter to transform text from the log file into structured data. Grok allows parsing input text using patterns. It ships with a number of different patterns, Apache being one of them. The file input transforms each line of the log file in an event. The Grok filter reads the events, finds the fields matching the pattern (ip, request type, response etc.) and adds them to the event. The Elasticsearch output gets the events, transforms them into JSON objects and sends it to an Elasticsearch instance. This is the configuration file for the pipeline: [gist 4e0c4561471f8a00f428] Change /path/to/apache_log to (guess what …) the path where apache log is in your machine and save it. If you don’t have an Apache log to test, use this one. Decompress it with tar -xzvf access_log.tgz You might have noticed that, contrary to what one could expect, the elasticsearch output does not specify the Elasticsearch instance’s address. Since Elasticsearch is running in the same machine as Logstash and is configured with Multicast enabled by default, it will be automatically found. This configuration is not recommended for production environments. Running Let’s start by initializing Elasticsearch: &lt;elasticsearch_root_folder&gt;/bin/elasticsearch -d Now Logstash: &lt;logstash_root_folder&gt;/bin/logstash -f pipeline.conf &amp; And now Kibana. Notice that we didn’t edit any configuration files for Kibana: by default, it will look for an Elasticsearch instance in localhost:9200 (which is exactly where our elasticsearch is). &lt;kibana_root_folder&gt;/bin/kibana &amp; Now browse to the port 5601 of your machine. You should see something like this: There you go! You have an open source, flexible and scalable log management system. You can read a little more about Kibana usage in this post.   [yasr_visitor_votes size=”small”]","author":{"@type":"Person","name":"Rafael M Teixeira"},"@type":"BlogPosting","url":"http://localhost:4000/en/2015/08/19/managing-logs-with-the-elk-stack/","headline":"Tutorial: Managing Logs with the ELK Stack","dateModified":"2015-08-20T01:53:32+02:00","datePublished":"2015-08-20T01:53:32+02:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/en/2015/08/19/managing-logs-with-the-elk-stack/"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Rafael M. Teixeira" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Rafael M. Teixeira</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Tutorial: Managing Logs with the ELK Stack</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2015-08-20T01:53:32+02:00" itemprop="datePublished">Aug 20, 2015
      </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">Rafael M Teixeira</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Consolidating, indexing and analyzing your environment’s logs is fundamental. Gone are the days when truckloads of logs were stored only to be used when something went wrong. It’s possible to extract a lot of information from your infrastructure and applications’ logs, allowing you to rapidly detect abnormalities, foresee problems and even support business decisions.</p>

<p>There are plenty of cloud log management services around: <a href="http://www.loggly.com" target="_blank">Loggly</a>, <a href="http://papertrailapp.com" target="_blank">Papertrail</a> e <a href="https://logentries.com/" target="_blank">Logentries</a> are some of the most used. Their goal is to be as simple as it gets: create an account, redirect your logs to the provided endpoint and that’s it: the rest is on their side. I’ve used these three and I might write about them some day, but we have another objective in this post: I’ll show how you can create a flexible and powerful log manager using the Elasticsearch/Logstash/Kibana stack.</p>

<!--more-->
<h2>ELK?</h2>
<p><a href="http://rafaelmt.github.io/wp-content/uploads/2015/08/logos1.png"><img class="wp-image-119 aligncenter" src="http://rafaelmt.github.io/wp-content/uploads/2015/08/logos1.png" alt="logos" width="578" height="263" /></a></p>

<p>All three components of the ELK stack are open source (<a href="https://tldrlegal.com/license/apache-license-2.0-(apache-2.0)">Apache 2 license</a>) and kept by Elastic, allowing modification, distribution and commercial use. The components are:</p>

<p><strong>Elasticsearch: </strong>distributed search engine based on Apache Lucene. It has a RESTful API as interface, with JSON objects.</p>

<p><strong>Logstash: </strong>data pipeline tool. Centralizes, normalizes and processes data from different sources.</p>

<p><strong>Kibana:</strong> data visualization web interface. Allows creating graphs and filters for data indexed by Elasticsearch.</p>

<p>This is the data flow throughout the stack: data is generated and sent to Logstash. Logstash reads, parses and filters the data, then sends it to Elasticsearch. Kibana uses Elasticsearch’s  indexes to search data and displays it in a user-friendly way.</p>

<p>In this example, we’ll use an Apache log as input. The flow is as follows:</p>

<p> </p>

<p><a href="http://rafaelmt.github.io/wp-content/uploads/2015/08/logstash-dataflow-1.png"><img class="size-full wp-image-74 aligncenter" src="http://rafaelmt.github.io/wp-content/uploads/2015/08/logstash-dataflow-1.png" alt="logstash-dataflow-1" width="523" height="309" /></a></p>
<h2>Installation</h2>
<p>I’ve used Centos 7 for this tutorial. The commands should work in any RHEL-based distro.</p>
<h3>Java</h3>
<p>Logstash runs both on Oracle JDK and OpenJDK. We’ll use OpenJDK:</p>

<p><code>sudo yum install java-1.8.0-openjdk.x86_64</code></p>

<p>The stack installation process is pretty straightforward: simply download the components and decompress the files. I’ll put them all in the /opt/ folder. I’m using the most recent versions (at the time of writing).</p>
<h3>Elasticsearch</h3>
<p><code>curl -O https://download.elastic.co/elasticsearch/elasticsearch/elasticsearch-1.7.1.tar.gz</code>
<code>sudo tar -xzvf elasticsearch-1.7.1.tar.gz -C /opt/</code></p>
<h3>Logstash</h3>
<p><code>curl -O https://download.elastic.co/logstash/logstash/logstash-1.5.3.tar.gz</code>
<code>sudo tar -xzvf logstash-1.5.3.tar.gz -C /opt/</code></p>
<h3>Kibana</h3>
<p><code>curl -O https://download.elastic.co/kibana/kibana/kibana-4.1.1-linux-x64.tar.gz</code>
<code>sudo tar -xzvf kibana-4.1.1-linux-x64.tar.gz -C /opt/</code></p>
<h2>Creating the Logstash pipeline</h2>
<p>To create a Logstash data pipeline, we need to specify the inputs, filters and outputs.  The inputs and outputs read and write data, while the filters adapt the data format to what the output expects. Logstash’s architecture allows installing inputs, filters and outputs as plugins, as well as creating your own.</p>

<p>Let’s create a pipeline with a an Apache access log as input and Elasticsearch as output. The scenario is as follows:</p>

<p><strong>input:</strong> reads a file containing Apache access log</p>

<p><strong>filter: </strong>parses each log line in a JSON object to be used as input in Elasticsearch</p>

<p><strong>output: </strong>sends the JSON objects to an Elasticsearch instance</p>

<p>We’ll use the <a href="https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html" target="_blank">Grok filter</a> to transform text from the log file into structured data. Grok allows parsing input text using patterns. It ships with a number of <a href="https://github.com/logstash-plugins/logstash-patterns-core/tree/master/patterns" target="_blank">different patterns</a>, Apache being one of them.</p>

<p>The <em>file</em> input transforms each line of the log file in an event. The Grok filter reads the events, finds the fields matching the pattern (ip, request type, response etc.) and adds them to the event. The Elasticsearch output gets the events, transforms them into JSON objects and sends it to an Elasticsearch instance.</p>

<p>This is the configuration file for the pipeline:
[gist 4e0c4561471f8a00f428]</p>

<p>Change /path/to/apache_log to (guess what …) the path where apache log is in your machine and save it. If you don’t have an Apache log to test, use <a href="http://rafaelmt.github.io/wp-content/uploads/2015/08/access_log.tgz">this one</a>. Decompress it with <code>tar -xzvf access_log.tgz</code></p>

<p>You might have noticed that, contrary to what one could expect, the elasticsearch output does not specify the Elasticsearch instance’s address. Since Elasticsearch is running in the same machine as Logstash and is configured with Multicast enabled by default, it will be automatically found. This configuration is <strong>not recommended for production environments.</strong></p>
<h2>Running</h2>
<p>Let’s start by initializing Elasticsearch:</p>

<p><code>&lt;elasticsearch_root_folder&gt;/bin/elasticsearch -d</code></p>

<p>Now Logstash:</p>

<p><code>&lt;logstash_root_folder&gt;/bin/logstash -f pipeline.conf &amp;</code></p>

<p>And now Kibana. Notice that we didn’t edit any configuration files for Kibana: by default, it will look for an Elasticsearch instance in localhost:9200 (which is exactly where our elasticsearch is).</p>

<p><code>&lt;kibana_root_folder&gt;/bin/kibana &amp;</code></p>

<p>Now browse to the port 5601 of your machine. You should see something like this:</p>

<p><a href="http://rafaelmt.github.io/wp-content/uploads/2015/08/Screen-Shot-2015-08-18-at-10.27.34-PM.png"><img class="alignnone size-full wp-image-71" src="http://rafaelmt.github.io/wp-content/uploads/2015/08/Screen-Shot-2015-08-18-at-10.27.34-PM.png" alt="kibana-dashboard" width="1280" height="627" /></a></p>

<p>There you go! You have an open source, flexible and scalable log management system. You can read a little more about Kibana usage <a href="http://rafaelmt.github.io/en/2015/09/01/kibana-tutorial/">in this post</a>.</p>

<p> </p>

<p>[yasr_visitor_votes size=”small”]</p>

  </div><a class="u-url" href="/en/2015/08/19/managing-logs-with-the-elk-stack/" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Rafael M. Teixeira</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Rafael M. Teixeira</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/rafaelmt"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">rafaelmt</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Half-stack developer, sometimes I write about stuff</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
