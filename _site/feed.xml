<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-04-18T23:34:00+02:00</updated><id>http://localhost:4000/</id><title type="html">Rafael M. Teixeira</title><subtitle>Half-stack developer, sometimes I write about stuff</subtitle><entry><title type="html">Don’t overengineer and get stuff done</title><link href="http://localhost:4000/en/2016/02/02/dont-overengineer-and-get-stuff-done/" rel="alternate" type="text/html" title="Don't overengineer and get stuff done" /><published>2016-02-02T15:33:49+01:00</published><updated>2016-02-02T15:33:49+01:00</updated><id>http://localhost:4000/en/2016/02/02/dont-overengineer-and-get-stuff-done</id><content type="html" xml:base="http://localhost:4000/en/2016/02/02/dont-overengineer-and-get-stuff-done/">&lt;p&gt;While reading Umberto Eco’s Numero Zero I came across a funny conversation that many software engineers will probably relate. Colonna, the main character and narrator, is talking to Romano Braggadocio, a journalist he just met. Braggadocio starts telling him about some story he is writing about that’s going to be so huge it might even turn into a book. There’s one thing, however, keeping him from finishing the required investigations: he needs a car.&lt;/p&gt;

&lt;p&gt;Braggadocio then proceeds to describe all the features he looks for in a car. It has to be fast, sturdy, not too wide so to not fit an average garage, but not too narrow so to not be tight for a big man like himself. It has to be low-maintenance and quickly available but not too quickly (this would mean nobody wants the car, he claims).  He enumerates a lot of candidate cars, but none fits the perfect model of car he envisions.&lt;/p&gt;

&lt;p&gt;As a result, his investigations never happen and the “big story” never comes through. The funny thing is, none of the features of the car would directly affect his investigations or keep him from doing them. You don’t need a car that goes from 0 to 100km/h in 7.5 seconds to run a few errands; one that does it in 8.5 seconds would do just fine.&lt;/p&gt;

&lt;p&gt;I saw an immediate parallel between this conversation and a number of personal/side projects that me and lots of colleagues have started but never finished. Everyone has cool ideas of new apps, web sites, services etc, but when it comes to implement it, some people get lost in an internal debate on whether they should use database A or B, which framework would scale better for 10 million users, what’s the best language for concurrency etc. While you do it, your idea is still just an idea.&lt;/p&gt;

&lt;p&gt;Engineers tend to overthink when they are starting new projects. As inherently lazy people, we dread the idea of having to rewrite stuff because the original implementation doesn’t scale well, or having to revamp the whole architecture because it does not support the load anymore. What we sometimes fail to grasp is that, in order to have those problems, the product has to exist. If you ever hit one of those performance walls, you’ll already have a somewhat successful product, which is certainly better than having just an idea.&lt;/p&gt;

&lt;p&gt;Don’t get me wrong, you can’t just randomly make decisions at the beginning of your project and hope for the best. Some rationale must be used to back your architectural/design choices. Just don’t let these decisions take too much of your time. Sam Altman from Y Combinator in his “&lt;a href=&quot;http://playbook.samaltman.com/&quot;&gt;Startup Playbook&lt;/a&gt;” (http://playbook.samaltman.com/) recommends building things that would work on a scale 10x bigger than your current one. I think this is a great rule of thumb.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt; &lt;/p&gt;</content><author><name>Rafael M Teixeira</name></author><summary type="html">While reading Umberto Eco’s Numero Zero I came across a funny conversation that many software engineers will probably relate. Colonna, the main character and narrator, is talking to Romano Braggadocio, a journalist he just met. Braggadocio starts telling him about some story he is writing about that’s going to be so huge it might even turn into a book. There’s one thing, however, keeping him from finishing the required investigations: he needs a car. Braggadocio then proceeds to describe all the features he looks for in a car. It has to be fast, sturdy, not too wide so to not fit an average garage, but not too narrow so to not be tight for a big man like himself. It has to be low-maintenance and quickly available but not too quickly (this would mean nobody wants the car, he claims).  He enumerates a lot of candidate cars, but none fits the perfect model of car he envisions. As a result, his investigations never happen and the “big story” never comes through. The funny thing is, none of the features of the car would directly affect his investigations or keep him from doing them. You don’t need a car that goes from 0 to 100km/h in 7.5 seconds to run a few errands; one that does it in 8.5 seconds would do just fine. I saw an immediate parallel between this conversation and a number of personal/side projects that me and lots of colleagues have started but never finished. Everyone has cool ideas of new apps, web sites, services etc, but when it comes to implement it, some people get lost in an internal debate on whether they should use database A or B, which framework would scale better for 10 million users, what’s the best language for concurrency etc. While you do it, your idea is still just an idea. Engineers tend to overthink when they are starting new projects. As inherently lazy people, we dread the idea of having to rewrite stuff because the original implementation doesn’t scale well, or having to revamp the whole architecture because it does not support the load anymore. What we sometimes fail to grasp is that, in order to have those problems, the product has to exist. If you ever hit one of those performance walls, you’ll already have a somewhat successful product, which is certainly better than having just an idea. Don’t get me wrong, you can’t just randomly make decisions at the beginning of your project and hope for the best. Some rationale must be used to back your architectural/design choices. Just don’t let these decisions take too much of your time. Sam Altman from Y Combinator in his “Startup Playbook” (http://playbook.samaltman.com/) recommends building things that would work on a scale 10x bigger than your current one. I think this is a great rule of thumb.    </summary></entry><entry><title type="html">Tutorial: Adding HTTPS to a WordPress website</title><link href="http://localhost:4000/en/2016/01/20/tutorial-adding-https-to-a-wordpress-website/" rel="alternate" type="text/html" title="Tutorial: Adding HTTPS to a WordPress website" /><published>2016-01-20T14:38:11+01:00</published><updated>2016-01-20T14:38:11+01:00</updated><id>http://localhost:4000/en/2016/01/20/tutorial-adding-https-to-a-wordpress-website</id><content type="html" xml:base="http://localhost:4000/en/2016/01/20/tutorial-adding-https-to-a-wordpress-website/">&lt;p&gt;Since Google &lt;a href=&quot;https://googlewebmastercentral.blogspot.com.br/2014/08/https-as-ranking-signal.html&quot;&gt;announced that it would boost the rank of websites with https&lt;/a&gt;, even websites that do not deal with sensitive data have a motivation to install a SSL certificate. In this tutorial I’ll go through the whole process of creation and installation of an SSL certificate to a self-hosted Wordpress website running on CentOS.&lt;/p&gt;
&lt;h2&gt;Purchasing the SSL Certificate&lt;/h2&gt;
&lt;p&gt;The first step is the purchase. You have to decide whether you want a wildcard certificate or not. The difference is that the wildcard certificate will certify all subdomains you may have, whilst the standard certificate will only cover the domain you provide. A wild card certificate would allow me to have https://db.rafaelmt.github.io for example, while the standard one would not.&lt;/p&gt;

&lt;p&gt;There are plenty of SSL certificate issuers around, many of them with very low cost for 1 year of the standard certificates. I went with GoDaddy, but the process should be similar for other providers. The purchase process is very straightforward. Don’t worry if you don’t get asked which domain you want to get certified at this point, you will provide this info later.&lt;/p&gt;
&lt;h2&gt;Generating the Request&lt;/h2&gt;
&lt;p&gt;Once you finished the checkout, you’ll be asked to provide a .csr file. This file is generated along with the private key of your certificate and it contains information about your domain, such as its name, location of your server etc.&lt;/p&gt;

&lt;p&gt;To generate the .csr file, execute the following command in your server:&lt;/p&gt;
&lt;pre&gt;openssl req -new -newkey rsa:2048 -nodes -keyout server.key -out server.csr&lt;/pre&gt;
&lt;p&gt;It will ask you to fill up the information about the certificate. When done, it will generate two files: the server.key, which is the private key of your certificate, and the server.csr, which is the file containing the request. The private key should not be shared with anyone! If you lose this file, you will have to re-generate the certificate.&lt;/p&gt;
&lt;h2&gt;Verify Domain Ownership&lt;/h2&gt;
&lt;p&gt;Your certificate issuer will ask you to verify the ownership of your domain. The method may vary, but the most common one is requesting you to create a file with a given file name in the root of your domain.&lt;/p&gt;

&lt;p&gt;Some certificate issuers will also request you to provide some documentation to verify your identity.&lt;/p&gt;
&lt;h2&gt;Download and Install Certificate&lt;/h2&gt;
&lt;p&gt;Once the domain ownership is verified, the issuer will allow you to download your certificate. You will probably get two .crt files, one containing the certificate (let’s call it server.crt), and another one containing the certificate chain. The certificate file is the one that has only one certificate block (&lt;code&gt;-----BEGIN CERTIFICATE----- ... -----END CERTIFICATE-----&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Copy the server.crt file to your server and place both server.crt and server.key in a place where Apache server can access. The typical location for the files are
&lt;code&gt;/etc/ssl/certs/server.crt
/etc/ssl/private/server.key
&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;Configure HTTPD&lt;/h2&gt;
&lt;p&gt;Add the following virtual host configuration to your /etc/httpd/conf/httpd.conf, changing the ServerName line to add your domain name:
[gist 1dd778987d936944bacf]
Make sure the following lines are present in your configuration:&lt;/p&gt;
&lt;pre&gt;LoadModule ssl_module modules/mod_ssl.so&lt;/pre&gt;
&lt;pre&gt;Listen 443 https&lt;/pre&gt;
&lt;p&gt;You also need to install the mod_ssl:&lt;/p&gt;
&lt;pre&gt;sudo yum install mod_ssl&lt;/pre&gt;
&lt;p&gt;Validate your configuration file by running the following command:&lt;/p&gt;
&lt;pre&gt;sudo service httpd configtest&lt;/pre&gt;
&lt;h2&gt;Restart Apache and Test!&lt;/h2&gt;
&lt;p&gt;Now let’s restart the apache server and test the https:&lt;/p&gt;
&lt;pre&gt;sudo service httpd restart&lt;/pre&gt;
&lt;p&gt;Fire up a browser and try to reach your server. Make sure you have opened the port 443 in iptables.&lt;/p&gt;
&lt;h2&gt;Redirect HTTP to HTTPS&lt;/h2&gt;
&lt;p&gt;Once you’re sure everything is working fine, you have to make sure users that access your website through http get redirected to https. To do so, add the following configuration to /etc/httpd/conf/httpd.conf:
[gist 67c8e639bef4cebd2e4d]&lt;/p&gt;</content><author><name>Rafael M Teixeira</name></author><summary type="html">Since Google announced that it would boost the rank of websites with https, even websites that do not deal with sensitive data have a motivation to install a SSL certificate. In this tutorial I’ll go through the whole process of creation and installation of an SSL certificate to a self-hosted Wordpress website running on CentOS. Purchasing the SSL Certificate The first step is the purchase. You have to decide whether you want a wildcard certificate or not. The difference is that the wildcard certificate will certify all subdomains you may have, whilst the standard certificate will only cover the domain you provide. A wild card certificate would allow me to have https://db.rafaelmt.github.io for example, while the standard one would not. There are plenty of SSL certificate issuers around, many of them with very low cost for 1 year of the standard certificates. I went with GoDaddy, but the process should be similar for other providers. The purchase process is very straightforward. Don’t worry if you don’t get asked which domain you want to get certified at this point, you will provide this info later. Generating the Request Once you finished the checkout, you’ll be asked to provide a .csr file. This file is generated along with the private key of your certificate and it contains information about your domain, such as its name, location of your server etc. To generate the .csr file, execute the following command in your server: openssl req -new -newkey rsa:2048 -nodes -keyout server.key -out server.csr It will ask you to fill up the information about the certificate. When done, it will generate two files: the server.key, which is the private key of your certificate, and the server.csr, which is the file containing the request. The private key should not be shared with anyone! If you lose this file, you will have to re-generate the certificate. Verify Domain Ownership Your certificate issuer will ask you to verify the ownership of your domain. The method may vary, but the most common one is requesting you to create a file with a given file name in the root of your domain. Some certificate issuers will also request you to provide some documentation to verify your identity. Download and Install Certificate Once the domain ownership is verified, the issuer will allow you to download your certificate. You will probably get two .crt files, one containing the certificate (let’s call it server.crt), and another one containing the certificate chain. The certificate file is the one that has only one certificate block (-----BEGIN CERTIFICATE----- ... -----END CERTIFICATE-----). Copy the server.crt file to your server and place both server.crt and server.key in a place where Apache server can access. The typical location for the files are /etc/ssl/certs/server.crt /etc/ssl/private/server.key Configure HTTPD Add the following virtual host configuration to your /etc/httpd/conf/httpd.conf, changing the ServerName line to add your domain name: [gist 1dd778987d936944bacf] Make sure the following lines are present in your configuration: LoadModule ssl_module modules/mod_ssl.so Listen 443 https You also need to install the mod_ssl: sudo yum install mod_ssl Validate your configuration file by running the following command: sudo service httpd configtest Restart Apache and Test! Now let’s restart the apache server and test the https: sudo service httpd restart Fire up a browser and try to reach your server. Make sure you have opened the port 443 in iptables. Redirect HTTP to HTTPS Once you’re sure everything is working fine, you have to make sure users that access your website through http get redirected to https. To do so, add the following configuration to /etc/httpd/conf/httpd.conf: [gist 67c8e639bef4cebd2e4d]</summary></entry><entry><title type="html">Having Multiple Databases under a Single Instance in RDS</title><link href="http://localhost:4000/en/2015/09/28/having-multiple-databases-under-a-single-instance-in-rds/" rel="alternate" type="text/html" title="Having Multiple Databases under a Single Instance in RDS" /><published>2015-09-28T20:07:42+02:00</published><updated>2015-09-28T20:07:42+02:00</updated><id>http://localhost:4000/en/2015/09/28/having-multiple-databases-under-a-single-instance-in-rds</id><content type="html" xml:base="http://localhost:4000/en/2015/09/28/having-multiple-databases-under-a-single-instance-in-rds/">&lt;p&gt;A not-so-well documented feature in AWS RDS is the ability of creating multiple databases under a single database instance. This can be a low-cost solution for test environments, since RDS charges per instance. You can lump together multiple test databases into a single instance and save some bucks, for example.&lt;/p&gt;

&lt;p&gt;Why not production? All instance resources will be shared among the databases, causing one database to affect the performance of the others. Another issue would be the database parameter group, which would have to be the same for all databases under the instance. All RDS backups are also instance-level, so you would be out of luck if you needed a backup for one of the databases only. The AWS dashboard does not seem to support this either, so you won’t see a list of the databases under the instance.&lt;/p&gt;
&lt;h3&gt;How To&lt;/h3&gt;
&lt;p&gt;There’s no way of creating other databases from the AWS dashboard or even from the API. You will have to rely on the DB engine’s commands to do it. For PostgreSQL, for example, connect to the instance (psql -h &amp;lt;RDS_endpoint&amp;gt; -U &amp;lt;master_user&amp;gt;) and run the following command:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;CREATE DATABASE &amp;lt;new_db_name&amp;gt;;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;From now on, you can connect to this DB with the regular psql command. To list the databases under the current instance, run &lt;code&gt;\list&lt;/code&gt; (or &lt;code&gt;\l&lt;/code&gt;). To switch databases, use &lt;code&gt;\connect &amp;lt;database_name&amp;gt;&lt;/code&gt;.&lt;/p&gt;</content><author><name>Rafael M Teixeira</name></author><summary type="html">A not-so-well documented feature in AWS RDS is the ability of creating multiple databases under a single database instance. This can be a low-cost solution for test environments, since RDS charges per instance. You can lump together multiple test databases into a single instance and save some bucks, for example. Why not production? All instance resources will be shared among the databases, causing one database to affect the performance of the others. Another issue would be the database parameter group, which would have to be the same for all databases under the instance. All RDS backups are also instance-level, so you would be out of luck if you needed a backup for one of the databases only. The AWS dashboard does not seem to support this either, so you won’t see a list of the databases under the instance. How To There’s no way of creating other databases from the AWS dashboard or even from the API. You will have to rely on the DB engine’s commands to do it. For PostgreSQL, for example, connect to the instance (psql -h &amp;lt;RDS_endpoint&amp;gt; -U &amp;lt;master_user&amp;gt;) and run the following command: CREATE DATABASE &amp;lt;new_db_name&amp;gt;; From now on, you can connect to this DB with the regular psql command. To list the databases under the current instance, run \list (or \l). To switch databases, use \connect &amp;lt;database_name&amp;gt;.</summary></entry><entry><title type="html">Automatically Shutting Down Idle AWS Linux Instances</title><link href="http://localhost:4000/en/2015/09/01/automatically-shutting-down-aws-linux-instances/" rel="alternate" type="text/html" title="Automatically Shutting Down Idle AWS Linux Instances" /><published>2015-09-01T02:49:09+02:00</published><updated>2015-09-01T02:49:09+02:00</updated><id>http://localhost:4000/en/2015/09/01/automatically-shutting-down-aws-linux-instances</id><content type="html" xml:base="http://localhost:4000/en/2015/09/01/automatically-shutting-down-aws-linux-instances/">&lt;p&gt;Let me guess: you forgot to turn off an EC2 instance that you’ve created to run a quick test only. Depending on how big the instance is and how long it took you to realize, it can be a costly mistake. And if you memory is as bad as mine, you have been there a couple of times …&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;To solve this problem, I created a script that turns off the machine after a given time without any SSH sessions:&lt;/p&gt;

&lt;p&gt;[gist 20a89197200f95d30b38]&lt;/p&gt;

&lt;p&gt;The time is passed as a parameter (in minutes). I haven’t tested in all distros, but it should be fine with RHEL-based ones (Centos, Amazon Linux etc). Just put it in the root’s crontab with a higher frequency than the desired timeout.. To check every minute and shut down after 30 minutes, for example, add the following line to root’s crontab (&lt;code&gt;sudo crontab -e&lt;/code&gt;):&lt;/p&gt;

&lt;p&gt;&lt;code&gt;*/1 * * * * /home/centos/autoshutdown.sh 30&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Change the path to where the script is saved. Also make sure you have the permission to execute it&lt;/p&gt;

&lt;p&gt;And please don’t forget to remove it before going to production!&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;[yasr_visitor_votes size=”small”]&lt;/p&gt;</content><author><name>Rafael M Teixeira</name></author><category term="aws" /><category term="linux" /><category term="script" /><summary type="html">Let me guess: you forgot to turn off an EC2 instance that you’ve created to run a quick test only. Depending on how big the instance is and how long it took you to realize, it can be a costly mistake. And if you memory is as bad as mine, you have been there a couple of times … To solve this problem, I created a script that turns off the machine after a given time without any SSH sessions: [gist 20a89197200f95d30b38] The time is passed as a parameter (in minutes). I haven’t tested in all distros, but it should be fine with RHEL-based ones (Centos, Amazon Linux etc). Just put it in the root’s crontab with a higher frequency than the desired timeout.. To check every minute and shut down after 30 minutes, for example, add the following line to root’s crontab (sudo crontab -e): */1 * * * * /home/centos/autoshutdown.sh 30 Change the path to where the script is saved. Also make sure you have the permission to execute it And please don’t forget to remove it before going to production!   [yasr_visitor_votes size=”small”]</summary></entry><entry><title type="html">Kibana Tutorial</title><link href="http://localhost:4000/en/2015/09/01/kibana-tutorial/" rel="alternate" type="text/html" title="Kibana Tutorial" /><published>2015-09-01T02:30:20+02:00</published><updated>2015-09-01T02:30:20+02:00</updated><id>http://localhost:4000/en/2015/09/01/kibana-tutorial</id><content type="html" xml:base="http://localhost:4000/en/2015/09/01/kibana-tutorial/">&lt;p&gt;In the &lt;a href=&quot;http://rafaelmt.github.io/en/2015/08/19/managing-logs-with-the-elk-stack/&quot;&gt;Managing Logs with the ELK Stack&lt;/a&gt; post, we have installed and configured an ELK stack to consolidate and analyze logs from an Apache Server. In this post I’ll talk a little more about Kibana and how to use it to create data charts and filter data.&lt;/p&gt;

&lt;!--more--&gt;
&lt;h2&gt;Getting Started&lt;/h2&gt;
&lt;p&gt;&lt;a id=&quot;refresh-fields&quot;&gt;&lt;/a&gt;First step is reloading the fields. This step is necessary so Kibana knows that the Apache log fields are indexed and can be used for searches. To do so, go to Settings -&amp;gt; Indexes. In the left-hand bar you’ll see all Elasticsearch indexes Kibana will load. This allows us to restrict Kibana’s access to the analyzed indexes only. Kibana comes with the Logstash index’s name pattern by default. If it does not happen, click “&lt;em&gt;Add new”&lt;/em&gt; and create an index with &lt;em&gt;logstash-* &lt;/em&gt;as pattern.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://rafaelmt.github.io/wp-content/uploads/2015/08/1-reindex.png&quot;&gt;&lt;img class=&quot;wp-image-105 aligncenter&quot; src=&quot;http://rafaelmt.github.io/wp-content/uploads/2015/08/1-reindex.png&quot; alt=&quot;1-reindex&quot; width=&quot;475&quot; height=&quot;201&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To reload the fields, choose Logstash index and press the orange &lt;em&gt;refresh&lt;/em&gt; button on the top right corner. The Apache log fields (path, request, agent etc.) should be displayed as &lt;em&gt;indexed&lt;/em&gt; in the field list.&lt;/p&gt;

&lt;p&gt;Let’s switch to the “Discover” tab. You will see a chart containing the amount of entries per day and the apache log entries in unparsed text. If you don’t see those, increase the analysis period on the top right corner. If logs still don’t appear, check if they are correctly loaded in Elasticsearch with the following commands:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;curl 'localhost:9200/_cat/indices?v'&lt;/code&gt; - List all Elasticsearch indexes&lt;/p&gt;

&lt;p&gt;&lt;code&gt;curl -XGET 'localhost:9200/&amp;lt;nome_do_indice&amp;gt;/_search?'&lt;/code&gt; -  List all entries in an index.&lt;/p&gt;

&lt;p&gt;The index’ fields are listed in the left-hand tab. Clicking them will show a count of occurrences. You can display the selected fields only instead of log text by clicking “&lt;em&gt;add&lt;/em&gt;”.&lt;/p&gt;

&lt;p&gt;If you see ”This field is not indexed thus unavailable for visualization and search” ou ”unindexed fields cannot be searched” when trying to add the Apache fields you need to &lt;a href=&quot;#refresh-fields&quot;&gt;reload the fields&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Creating Visualizations&lt;/h2&gt;
&lt;p&gt;To create a vertical bar graph containing the count of every value in a field, select it and click “Visualize”:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://rafaelmt.github.io/wp-content/uploads/2015/08/agent-chart.png&quot;&gt;&lt;img class=&quot; wp-image-110 alignnone&quot; src=&quot;http://rafaelmt.github.io/wp-content/uploads/2015/08/agent-chart.png&quot; alt=&quot;agent-chart&quot; width=&quot;695&quot; height=&quot;341&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Save the visualization by clicking the top right corner &lt;em&gt;save&lt;/em&gt; button.&lt;/p&gt;

&lt;p&gt;There are other forms of data visualization. In the &lt;em&gt;Visualize&lt;/em&gt; you will see a list of possible visualizations and its use cases. Let’s create another visualization to count the errors 500 returned by the server. Select the “Metric” visualization and in the next step choose “From a new search”.&lt;/p&gt;

&lt;p&gt;Kibana’s search syntax is the same as Elasticsearch’s (with is the same as Apache Lucene’s). The &lt;code&gt;response: 500&lt;/code&gt; query returns the responses with code 500. Replace * for this query. Save it in another visualization.&lt;/p&gt;
&lt;h2&gt;Creating a Dashboard&lt;/h2&gt;
&lt;p&gt;Change to the Dashboard tab. By clicking + in the top right corner, you can add visualizations to your Dashboard. They can be re-dimensioned and re-organized.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://rafaelmt.github.io/wp-content/uploads/2015/08/Screen-Shot-2015-08-25-at-10.56.34-AM.png&quot;&gt;&lt;img class=&quot;wp-image-112 alignnone&quot; src=&quot;http://rafaelmt.github.io/wp-content/uploads/2015/08/Screen-Shot-2015-08-25-at-10.56.34-AM.png&quot; alt=&quot;Screen Shot 2015-08-25 at 10.56.34 AM&quot; width=&quot;727&quot; height=&quot;356&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;[yasr_visitor_votes size=”small”]&lt;/p&gt;</content><author><name>Rafael M Teixeira</name></author><summary type="html">In the Managing Logs with the ELK Stack post, we have installed and configured an ELK stack to consolidate and analyze logs from an Apache Server. In this post I’ll talk a little more about Kibana and how to use it to create data charts and filter data. Getting Started First step is reloading the fields. This step is necessary so Kibana knows that the Apache log fields are indexed and can be used for searches. To do so, go to Settings -&amp;gt; Indexes. In the left-hand bar you’ll see all Elasticsearch indexes Kibana will load. This allows us to restrict Kibana’s access to the analyzed indexes only. Kibana comes with the Logstash index’s name pattern by default. If it does not happen, click “Add new” and create an index with logstash-* as pattern. To reload the fields, choose Logstash index and press the orange refresh button on the top right corner. The Apache log fields (path, request, agent etc.) should be displayed as indexed in the field list. Let’s switch to the “Discover” tab. You will see a chart containing the amount of entries per day and the apache log entries in unparsed text. If you don’t see those, increase the analysis period on the top right corner. If logs still don’t appear, check if they are correctly loaded in Elasticsearch with the following commands: curl 'localhost:9200/_cat/indices?v' - List all Elasticsearch indexes curl -XGET 'localhost:9200/&amp;lt;nome_do_indice&amp;gt;/_search?' -  List all entries in an index. The index’ fields are listed in the left-hand tab. Clicking them will show a count of occurrences. You can display the selected fields only instead of log text by clicking “add”. If you see ”This field is not indexed thus unavailable for visualization and search” ou ”unindexed fields cannot be searched” when trying to add the Apache fields you need to reload the fields Creating Visualizations To create a vertical bar graph containing the count of every value in a field, select it and click “Visualize”: Save the visualization by clicking the top right corner save button. There are other forms of data visualization. In the Visualize you will see a list of possible visualizations and its use cases. Let’s create another visualization to count the errors 500 returned by the server. Select the “Metric” visualization and in the next step choose “From a new search”. Kibana’s search syntax is the same as Elasticsearch’s (with is the same as Apache Lucene’s). The response: 500 query returns the responses with code 500. Replace * for this query. Save it in another visualization. Creating a Dashboard Change to the Dashboard tab. By clicking + in the top right corner, you can add visualizations to your Dashboard. They can be re-dimensioned and re-organized.   [yasr_visitor_votes size=”small”]</summary></entry><entry><title type="html">Tutorial: Managing Logs with the ELK Stack</title><link href="http://localhost:4000/en/2015/08/19/managing-logs-with-the-elk-stack/" rel="alternate" type="text/html" title="Tutorial: Managing Logs with the ELK Stack" /><published>2015-08-20T01:53:32+02:00</published><updated>2015-08-20T01:53:32+02:00</updated><id>http://localhost:4000/en/2015/08/19/managing-logs-with-the-elk-stack</id><content type="html" xml:base="http://localhost:4000/en/2015/08/19/managing-logs-with-the-elk-stack/">&lt;p&gt;Consolidating, indexing and analyzing your environment’s logs is fundamental. Gone are the days when truckloads of logs were stored only to be used when something went wrong. It’s possible to extract a lot of information from your infrastructure and applications’ logs, allowing you to rapidly detect abnormalities, foresee problems and even support business decisions.&lt;/p&gt;

&lt;p&gt;There are plenty of cloud log management services around: &lt;a href=&quot;http://www.loggly.com&quot; target=&quot;_blank&quot;&gt;Loggly&lt;/a&gt;, &lt;a href=&quot;http://papertrailapp.com&quot; target=&quot;_blank&quot;&gt;Papertrail&lt;/a&gt; e &lt;a href=&quot;https://logentries.com/&quot; target=&quot;_blank&quot;&gt;Logentries&lt;/a&gt; are some of the most used. Their goal is to be as simple as it gets: create an account, redirect your logs to the provided endpoint and that’s it: the rest is on their side. I’ve used these three and I might write about them some day, but we have another objective in this post: I’ll show how you can create a flexible and powerful log manager using the Elasticsearch/Logstash/Kibana stack.&lt;/p&gt;

&lt;!--more--&gt;
&lt;h2&gt;ELK?&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://rafaelmt.github.io/wp-content/uploads/2015/08/logos1.png&quot;&gt;&lt;img class=&quot;wp-image-119 aligncenter&quot; src=&quot;http://rafaelmt.github.io/wp-content/uploads/2015/08/logos1.png&quot; alt=&quot;logos&quot; width=&quot;578&quot; height=&quot;263&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;All three components of the ELK stack are open source (&lt;a href=&quot;https://tldrlegal.com/license/apache-license-2.0-(apache-2.0)&quot;&gt;Apache 2 license&lt;/a&gt;) and kept by Elastic, allowing modification, distribution and commercial use. The components are:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Elasticsearch: &lt;/strong&gt;distributed search engine based on Apache Lucene. It has a RESTful API as interface, with JSON objects.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Logstash: &lt;/strong&gt;data pipeline tool. Centralizes, normalizes and processes data from different sources.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kibana:&lt;/strong&gt; data visualization web interface. Allows creating graphs and filters for data indexed by Elasticsearch.&lt;/p&gt;

&lt;p&gt;This is the data flow throughout the stack: data is generated and sent to Logstash. Logstash reads, parses and filters the data, then sends it to Elasticsearch. Kibana uses Elasticsearch’s  indexes to search data and displays it in a user-friendly way.&lt;/p&gt;

&lt;p&gt;In this example, we’ll use an Apache log as input. The flow is as follows:&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://rafaelmt.github.io/wp-content/uploads/2015/08/logstash-dataflow-1.png&quot;&gt;&lt;img class=&quot;size-full wp-image-74 aligncenter&quot; src=&quot;http://rafaelmt.github.io/wp-content/uploads/2015/08/logstash-dataflow-1.png&quot; alt=&quot;logstash-dataflow-1&quot; width=&quot;523&quot; height=&quot;309&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Installation&lt;/h2&gt;
&lt;p&gt;I’ve used Centos 7 for this tutorial. The commands should work in any RHEL-based distro.&lt;/p&gt;
&lt;h3&gt;Java&lt;/h3&gt;
&lt;p&gt;Logstash runs both on Oracle JDK and OpenJDK. We’ll use OpenJDK:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;sudo yum install java-1.8.0-openjdk.x86_64&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The stack installation process is pretty straightforward: simply download the components and decompress the files. I’ll put them all in the /opt/ folder. I’m using the most recent versions (at the time of writing).&lt;/p&gt;
&lt;h3&gt;Elasticsearch&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;curl -O https://download.elastic.co/elasticsearch/elasticsearch/elasticsearch-1.7.1.tar.gz&lt;/code&gt;
&lt;code&gt;sudo tar -xzvf elasticsearch-1.7.1.tar.gz -C /opt/&lt;/code&gt;&lt;/p&gt;
&lt;h3&gt;Logstash&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;curl -O https://download.elastic.co/logstash/logstash/logstash-1.5.3.tar.gz&lt;/code&gt;
&lt;code&gt;sudo tar -xzvf logstash-1.5.3.tar.gz -C /opt/&lt;/code&gt;&lt;/p&gt;
&lt;h3&gt;Kibana&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;curl -O https://download.elastic.co/kibana/kibana/kibana-4.1.1-linux-x64.tar.gz&lt;/code&gt;
&lt;code&gt;sudo tar -xzvf kibana-4.1.1-linux-x64.tar.gz -C /opt/&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;Creating the Logstash pipeline&lt;/h2&gt;
&lt;p&gt;To create a Logstash data pipeline, we need to specify the inputs, filters and outputs.  The inputs and outputs read and write data, while the filters adapt the data format to what the output expects. Logstash’s architecture allows installing inputs, filters and outputs as plugins, as well as creating your own.&lt;/p&gt;

&lt;p&gt;Let’s create a pipeline with a an Apache access log as input and Elasticsearch as output. The scenario is as follows:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;input:&lt;/strong&gt; reads a file containing Apache access log&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;filter: &lt;/strong&gt;parses each log line in a JSON object to be used as input in Elasticsearch&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;output: &lt;/strong&gt;sends the JSON objects to an Elasticsearch instance&lt;/p&gt;

&lt;p&gt;We’ll use the &lt;a href=&quot;https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html&quot; target=&quot;_blank&quot;&gt;Grok filter&lt;/a&gt; to transform text from the log file into structured data. Grok allows parsing input text using patterns. It ships with a number of &lt;a href=&quot;https://github.com/logstash-plugins/logstash-patterns-core/tree/master/patterns&quot; target=&quot;_blank&quot;&gt;different patterns&lt;/a&gt;, Apache being one of them.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;file&lt;/em&gt; input transforms each line of the log file in an event. The Grok filter reads the events, finds the fields matching the pattern (ip, request type, response etc.) and adds them to the event. The Elasticsearch output gets the events, transforms them into JSON objects and sends it to an Elasticsearch instance.&lt;/p&gt;

&lt;p&gt;This is the configuration file for the pipeline:
[gist 4e0c4561471f8a00f428]&lt;/p&gt;

&lt;p&gt;Change /path/to/apache_log to (guess what …) the path where apache log is in your machine and save it. If you don’t have an Apache log to test, use &lt;a href=&quot;http://rafaelmt.github.io/wp-content/uploads/2015/08/access_log.tgz&quot;&gt;this one&lt;/a&gt;. Decompress it with &lt;code&gt;tar -xzvf access_log.tgz&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;You might have noticed that, contrary to what one could expect, the elasticsearch output does not specify the Elasticsearch instance’s address. Since Elasticsearch is running in the same machine as Logstash and is configured with Multicast enabled by default, it will be automatically found. This configuration is &lt;strong&gt;not recommended for production environments.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Running&lt;/h2&gt;
&lt;p&gt;Let’s start by initializing Elasticsearch:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;&amp;lt;elasticsearch_root_folder&amp;gt;/bin/elasticsearch -d&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Now Logstash:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;&amp;lt;logstash_root_folder&amp;gt;/bin/logstash -f pipeline.conf &amp;amp;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;And now Kibana. Notice that we didn’t edit any configuration files for Kibana: by default, it will look for an Elasticsearch instance in localhost:9200 (which is exactly where our elasticsearch is).&lt;/p&gt;

&lt;p&gt;&lt;code&gt;&amp;lt;kibana_root_folder&amp;gt;/bin/kibana &amp;amp;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Now browse to the port 5601 of your machine. You should see something like this:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://rafaelmt.github.io/wp-content/uploads/2015/08/Screen-Shot-2015-08-18-at-10.27.34-PM.png&quot;&gt;&lt;img class=&quot;alignnone size-full wp-image-71&quot; src=&quot;http://rafaelmt.github.io/wp-content/uploads/2015/08/Screen-Shot-2015-08-18-at-10.27.34-PM.png&quot; alt=&quot;kibana-dashboard&quot; width=&quot;1280&quot; height=&quot;627&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;There you go! You have an open source, flexible and scalable log management system. You can read a little more about Kibana usage &lt;a href=&quot;http://rafaelmt.github.io/en/2015/09/01/kibana-tutorial/&quot;&gt;in this post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;[yasr_visitor_votes size=”small”]&lt;/p&gt;</content><author><name>Rafael M Teixeira</name></author><category term="elasticsearch" /><category term="elk" /><category term="kibana" /><category term="logstash" /><category term="tutorial" /><summary type="html">Consolidating, indexing and analyzing your environment’s logs is fundamental. Gone are the days when truckloads of logs were stored only to be used when something went wrong. It’s possible to extract a lot of information from your infrastructure and applications’ logs, allowing you to rapidly detect abnormalities, foresee problems and even support business decisions. There are plenty of cloud log management services around: Loggly, Papertrail e Logentries are some of the most used. Their goal is to be as simple as it gets: create an account, redirect your logs to the provided endpoint and that’s it: the rest is on their side. I’ve used these three and I might write about them some day, but we have another objective in this post: I’ll show how you can create a flexible and powerful log manager using the Elasticsearch/Logstash/Kibana stack. ELK? All three components of the ELK stack are open source (Apache 2 license) and kept by Elastic, allowing modification, distribution and commercial use. The components are: Elasticsearch: distributed search engine based on Apache Lucene. It has a RESTful API as interface, with JSON objects. Logstash: data pipeline tool. Centralizes, normalizes and processes data from different sources. Kibana: data visualization web interface. Allows creating graphs and filters for data indexed by Elasticsearch. This is the data flow throughout the stack: data is generated and sent to Logstash. Logstash reads, parses and filters the data, then sends it to Elasticsearch. Kibana uses Elasticsearch’s  indexes to search data and displays it in a user-friendly way. In this example, we’ll use an Apache log as input. The flow is as follows:   Installation I’ve used Centos 7 for this tutorial. The commands should work in any RHEL-based distro. Java Logstash runs both on Oracle JDK and OpenJDK. We’ll use OpenJDK: sudo yum install java-1.8.0-openjdk.x86_64 The stack installation process is pretty straightforward: simply download the components and decompress the files. I’ll put them all in the /opt/ folder. I’m using the most recent versions (at the time of writing). Elasticsearch curl -O https://download.elastic.co/elasticsearch/elasticsearch/elasticsearch-1.7.1.tar.gz sudo tar -xzvf elasticsearch-1.7.1.tar.gz -C /opt/ Logstash curl -O https://download.elastic.co/logstash/logstash/logstash-1.5.3.tar.gz sudo tar -xzvf logstash-1.5.3.tar.gz -C /opt/ Kibana curl -O https://download.elastic.co/kibana/kibana/kibana-4.1.1-linux-x64.tar.gz sudo tar -xzvf kibana-4.1.1-linux-x64.tar.gz -C /opt/ Creating the Logstash pipeline To create a Logstash data pipeline, we need to specify the inputs, filters and outputs.  The inputs and outputs read and write data, while the filters adapt the data format to what the output expects. Logstash’s architecture allows installing inputs, filters and outputs as plugins, as well as creating your own. Let’s create a pipeline with a an Apache access log as input and Elasticsearch as output. The scenario is as follows: input: reads a file containing Apache access log filter: parses each log line in a JSON object to be used as input in Elasticsearch output: sends the JSON objects to an Elasticsearch instance We’ll use the Grok filter to transform text from the log file into structured data. Grok allows parsing input text using patterns. It ships with a number of different patterns, Apache being one of them. The file input transforms each line of the log file in an event. The Grok filter reads the events, finds the fields matching the pattern (ip, request type, response etc.) and adds them to the event. The Elasticsearch output gets the events, transforms them into JSON objects and sends it to an Elasticsearch instance. This is the configuration file for the pipeline: [gist 4e0c4561471f8a00f428] Change /path/to/apache_log to (guess what …) the path where apache log is in your machine and save it. If you don’t have an Apache log to test, use this one. Decompress it with tar -xzvf access_log.tgz You might have noticed that, contrary to what one could expect, the elasticsearch output does not specify the Elasticsearch instance’s address. Since Elasticsearch is running in the same machine as Logstash and is configured with Multicast enabled by default, it will be automatically found. This configuration is not recommended for production environments. Running Let’s start by initializing Elasticsearch: &amp;lt;elasticsearch_root_folder&amp;gt;/bin/elasticsearch -d Now Logstash: &amp;lt;logstash_root_folder&amp;gt;/bin/logstash -f pipeline.conf &amp;amp; And now Kibana. Notice that we didn’t edit any configuration files for Kibana: by default, it will look for an Elasticsearch instance in localhost:9200 (which is exactly where our elasticsearch is). &amp;lt;kibana_root_folder&amp;gt;/bin/kibana &amp;amp; Now browse to the port 5601 of your machine. You should see something like this: There you go! You have an open source, flexible and scalable log management system. You can read a little more about Kibana usage in this post.   [yasr_visitor_votes size=”small”]</summary></entry></feed>